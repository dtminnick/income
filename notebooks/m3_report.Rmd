---
title: "Predicting Income Level from Demographic and Behavioral Variables"
subtitle: "A Comparison of Linear Regression with an Indicator Matrix and Logistic
  Regression Approaches"
author: "Donnie Minnick, Statistical Learning - Fall A 2025"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  word_document: default
  html_document:
    df_print: paged
    theme:
      version: 4
      bootswatch: flatly
      primary: '#2C3E50'
      secondary: '#18BC9C'
      base_font: Arial
      heading_font: Calibri
---

# Project Overview

This project evaluates two modeling approaches, linear regression with an indicator matrix and logistic regression, to determine which more reliably classifies individuals as earning above or below $50,000 per year. 

Using the UCI Adult Income dataset, the comparison focuses on career stage, marital status, working hours, education level, and investment activity as the explanatory variables.

I compare model performance using practical measures, including accuracy, sensitivity, specificity, and Area Under the ROC Curve (AUC). 

My goal is to identify the method that not only provides stronger predictive capability but also offers clearer insight into how these three dimensions of socioeconomic behavior and opportunity contribute to income differences.

## Analysis Question

How do linear regression with an indicator matrix and logistic regression compare in their ability to classify individuals as earning above or below $50,000 per year, based on career stage, marital status, working hours, education level, and investment activity and other demographic and behavioral variables, in terms of accuracy, sensitivity, specificity, and AUC?

## Data Suitability

The UCI Adult Income dataset is well suited for this analysis based on the following characteristics:

**Binary Target Variable:** The income label is already dichotomized as `<=50K` and `>50K`, making it ideal for classification tasks.

**Relevant Predictors:** Career stage can be proxied through variables such as age, occupation, and years of work experience. Education level is directly available in multiple forms (years of education and categorical attainment). And investment activity can be approximated through features like capital gains and capital losses, which provide signals of financial activity beyond earned income.  Marital status and hours worked provide a view into socioeconomic dynamics influencing income level.

**Mixed Data Types:** These predictors combine categorical (education level, marital status, occupation) and continuous (capital gains, capital losses, age) data, allowing a comparison of how well each modeling method handles different variable types.

**Real-World Relevance:** The selected variables represent core levers of socioeconomic advancement, human capital (education), labor market position (career stage), and financial behavior (investment activity). Evaluating their relationship to income provides insights into drivers of upward mobility.

## Candidate Models

### Linear Regression with an Indicator Matrix

Linear regression, when applied with a binary indicator response, can be used to estimate probabilities of class membership. Although not traditionally designed for classification, it provides a straightforward benchmark and can reveal how continuous predictors like age or hours worked relate linearly to income. However, it may yield predictions outside the 0–1 range and does not naturally account for the probabilistic nature of binary outcomes.

### Logistic Regression

Logistic regression is a standard method for binary classification, modeling the log-odds of the outcome as a linear combination of predictors. It constrains predicted values between 0 and 1 and provides interpretable coefficients in terms of odds ratios. It is particularly well-suited for this problem and serves as the conventional baseline for evaluating newer or more complex classifiers.

### Rationale

Placing these two approaches side by side highlights the importance of choosing models that align with the data structure and analysis question. By comparing their performance on accuracy, sensitivity, specificity, and AUC, this analysis will demonstrate the trade-offs between a general-purpose regression method and a model purpose-built for classification.

### Baseline Expectations

Before conducting the analysis, it is important to establish expectations about how the candidate models are likely to perform:

**Linear Regression Benchmark:** Linear regression with an indicator matrix may provide a useful baseline, but its predictions can extend outside the valid probability range and may not align as well with classification thresholds. Accuracy may be reasonable, but sensitivity and specificity are likely to suffer compared to logistic regression.

**Logistic Regression Advantage:** Because logistic regression is specifically designed for binary classification, it is expected to outperform linear regression in terms of calibration and overall predictive reliability. Its ability to constrain predictions between 0 and 1 aligns naturally with the problem structure.

**Comparative Outlook:** Logistic regression is anticipated to deliver higher AUC and more balanced classification metrics, while linear regression may illustrate the pitfalls of applying a general-purpose model to a classification task. This contrast should highlight the importance of model choice in predictive analytics.

## Modeling Outcome Preview

To orient the reader, this section summarizes the final modeling decision, key performance metrics, and major challenges addressed throughout the pipeline.

### Final Model Choice

After benchmarking two classification approaches,linear regression with an indicator matrix and logistic regression, I selected a logistic regression model with class weighting as the final model. This choice offered a strong balance of interpretability, predictive performance, achieving a balanced accuracy of 0.82 on the test set and maintaining robust subgroup performance across income brackets.

### Key Performance Metrics

**Balanced Accuracy:** 0.82 (test set)

**Sensitivity / Specificity:** Well-balanced across classes

**Residual Diagnostics:** Logistic regression residuals showed no systematic bias or heteroscedasticity, supporting model adequacy. In contrast, linear regression residuals revealed non-normality and variance inflation, undermining reliability.

### Limitations of Linear Regression as a Classifier

Although linear regression can be adapted for binary classification by thresholding predicted values, it presents several structural limitations that undermine its suitability. Its unbounded predictions lack probabilistic meaning, violate key assumptions like homoscedasticity, and result in poor calibration. Moreover, the absence of a log-odds interpretation complicates communication and stakeholder understanding. These limitations, confirmed through residual diagnostics and model behavior, motivated a shift toward logistic regression, which provides bounded, interpretable outputs and a more principled foundation for classification.

### Challenges and Solutions

Key challenges included multicollinearity, class imbalance, and the limitations of linear regression as a classification tool. These were addressed through feature pruning, class weighting, and model selection grounded in diagnostic metrics. The sections that follow detail the full modeling pipeline, from data preparation to final evaluation.

## Github Repo and Source Data File

All project files are maintained in [this Github repository](https://github.com/dtminnick/income).

The UCI Adult Income dataset and related information are available for download from the [UCI archive site](https://archive.ics.uci.edu/dataset/2/adult).

## Code Libraries

My analysis leverages the following R packages: `caret` for model training and evaluation, `dplyr` and `tidyr` for data manipulation, `ggplot2` for plots, `knitr` for table formatting, and `pROC` for ROC/AUC analysis.

Two customized R functions enable evaluation of predictor associations and multicollinearity checks.

Code chunks are presented in this R markdown document alongside analysis to document implementation of analysis, model creation, and evaluation.

```{r, load_libraries}
library("caret")
library("dplyr")
library("ggplot2")
library("knitr")
library("pROC")
library("tidyr")

source("../R/check_multicollinearity_factors.R")
source("../R/test_predictor_associations.R")
```

# Data Exploration, Cleaning and Transformation

Exploratory analysis focuses on understanding five variables in the source data that will be transformed for analysis purposes. Three of the variables have a categorical version and numerical version to aid exploratory analysis.

Load the income data and select in-scope variables. Create a numeric version of the response variable.

```{r load_data}
income <- readRDS("../data/income.rds") %>%
  select(income, 
         age,
         marital_status,
         hours_per_week,
         education, 
         capital_gain, 
         capital_loss) %>%
  mutate(income_num = if_else(income == "<=50K", 0, 1))
```

## Missing Values

Generate a report summarizing missing values in the dataset at both the column and row level. Compute the total number and percentage of rows containing any missing values. And combine summaries into a single table for easy inspection and reporting.

```{r missing_data}
# Create column level summary.

col_missing <- income %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "missing_count") %>%
  mutate(missing_percent = round(missing_count / nrow(income), 2))

# Create row level summary.

row_missing <- tibble(variable = "rows_with_missing_data",
                      missing_count = sum(!complete.cases(income)),
                      missing_percent = round(sum(!complete.cases(income)) / nrow(income), 2))

# Combine summaries.

missing_report <- bind_rows(col_missing, row_missing)

# Generate formatted report.

kable(missing_report,
      col.names = c("Variable", "Missing Count", "Missing Percent"),
      format.args = list(big.mark = ","),
      align = c("l", "r", "r"))
```

The table confirms there are no rows with missing data in the reported rows and columns.  The source dataset contains 32,560 observations.

## Income (Response)

`income` is a binary categorical variable indicating whether an individual's annual income exceeds $50,000. It has two levels: `<=50K` for those earning $50,000 or less, and `>50K` for those earning more than $50,000.  

`income_num` is a numeric variable that corresponds to `income`, i.e. 0 represents the `<=50k` category and 1 represents the `>50k` category.

```{r eda_income}
# Generate a summary of the income variable.

income_summary <- income %>%
  group_by(income) %>%
  summarise(entries = n()) %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot the distribution of values.

ggplot(income_summary, aes(x = income, y = entries)) +
  geom_col(fill = "steelblue", color = "white") +
  geom_text(aes(label = paste0(scales::comma(entries), "\n", round(percent * 100, 0), "%")),
            vjust = -0.3, size = 3.0) +
  labs(title = "Distribution of Income",
       x = "Income",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(income_summary$entries) * 1.2)
```

This distribution is a classic case of class imbalance and it’s substantial: 76% of individuals fall into the `<=50K` category, while only 24% belong to the `>50K` category.

Given the potential baseline accuracy trap, i.e. because the `<=50K` class dominates the dataset at 76%, I can achieve high accuracy simply by predicting the majority class every time. But this doesn't provide a meaningful model.

I need to also measure how well the models detect the minority class, ensure that I am not over-predicting the majority class, and generate models with overall discriminatory power.  I will use weights as a means to balance the classes before training the models.

Encode factor levels for income, using `<=50K` as the reference level.

```{r}
# Make response variable a factor.

income <- income %>%
  mutate(income = factor(income, levels = c("<=50K", ">50K")),
         income_num = factor(income_num, levels = c(0, 1)))
```

## Age

`age` is a continuous numeric variable, ranging from 17 to 90.  The following plot generates a summary of the `age` variable.

```{r}
# Generate a summary of the age variable.

age_summary <- income %>%
  group_by(age) %>%
  summarise(entries = n()) %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot the distribution of values.

ggplot(age_summary, aes(x = age, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Age",
       x = "Age",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

This distribution reflects what we'd expect from a working-age population in the US: a peak around ages 20-30, reflecting a large cohort entering the workforce, a gradual decline beginning at age 37-38 through age 55, typical of aging out of peak earning years or shifting to preparation for retirement, and dropoff after ages 60-50, reflecting retirement and reduced representation.

Age is likely nonlinear in its relationship to income class.  Use binning to confirm this dynamic.

```{r}
# Create age bins.

income_age <- income %>%
  mutate(age_bin = cut(age, breaks = seq(15, 90, by = 5), include.lowest = TRUE))

# Calculate proportions within each age bin and income group.

age_income_prop <- income_age %>%
  group_by(age_bin, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(age_bin) %>%
  mutate(prop = count / sum(count))

# Generate plot.

ggplot(age_income_prop, aes(x = age_bin, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Proportion of Income Classes by Age Group",
       x = "Age Group",
       y = "Proportion",
       fill = "Income Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

This plot shows that income probability doesn't increase linearly with age; rather, it peaks mid-career and then drops.  I'll use a binning strategy to model this non-monotonic pattern in a `career_stage` variable, i.e. instead of assuming that each year of age adds the same effect, binning allows me to treat age as a set of behavioral groups. To better capture this pattern, I will engineer a career stage feature.

## Career Stage

The `career_stage` feature will contain four groups: 1) entry-level (ages 17-30), 2) growth phase (ages 31-45), 3) peak earning (ages 46-60), and 4) retirement transition (ages 61+).  Encode factor levels for career stage, using Entry-level as the reference level.

```{r define_career_stage}
# Create career_stage variable.

income <- income %>%
  mutate(career_stage = case_when(
    age < 31 ~ "Entry-level",
    age >= 21 & age < 45 ~ "Growth-phase",
    age >= 36 & age < 61 ~ "Peak-earning",
    age >= 61~ "Retirement-transition"))

# Make variable a factor.

income <- income %>%
  mutate(career_stage = factor(career_stage, 
                               ordered = TRUE,
                               levels = c("Entry-level", 
                                          "Growth-phase", 
                                          "Peak-earning", 
                                          "Retirement-transition")))
```

Visually confirm the non-linear effects of career stage on income classification.

```{r}
# Group by career stage.

career_stage_prop <- income %>%
  group_by(career_stage, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(career_stage) %>%
  mutate(prop = count / sum(count))

# Generate plot.

ggplot(career_stage_prop, aes(x = career_stage, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Proportion of Income Classes by Career Stage",
       x = "Career Stage",
       y = "Proportion",
       fill = "Income Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

To capture this nonlinear effect, I will test a second-degree orthogonal polynomial transformation of the numeric career stage variable as `poly(career_stage_num, 2)`.  It will model both linear and quadratic trends while avoiding multicollinearity.

There is a single inflection point in this distribution. The chart shows a rise in `>50K` income class from Entry-level to Peak-earning, followed by a decline in Retirement-transition. That’s a classic parabolic shape with one turning point which a quadratic models naturally.

This should improve numerical stability and interpretability of regression coefficients, especially when modeling non-monotonic relationships (e.g., mid-career income peaks followed by retirement declines).

## Marital Status

The marital status variable captures an individual's relationship status and is a categorical feature with the following levels:

* Married-civ-spouse: Legally married and living with spouse,
* Married-AF-spouse: Married to a spouse in the armed forces,
* Divorced: Legally separated after marriage,
* Separated: Still legally married but not living together,
* Widowed: Spouse has passed away, and
* Never-married.

Generate a summary of `marital_status`.

```{r}
# Generate summary.

marital_status_summary <- income %>%
  group_by(marital_status) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot distribution of values.

ggplot(marital_status_summary, aes(x = forcats::fct_reorder(marital_status, entries, .desc = TRUE), y = entries)) +
  geom_col(fill = "steelblue", color = "white") +
  geom_text(aes(label = paste0(scales::comma(entries), "\n", round(percent * 100, 0), "%")),
            vjust = -0.3, size = 3.0) +
  labs(title = "Distribution of Marital Status",
       x = "Marital Status",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(marital_status_summary$entries) * 1.2)
```

Married-civ-spouse dominates at 46%.  Never-married follows at 33%.  The rest of the categories are smaller slices, but potentially behaviorally distinct.

Marital status could be a strong socioeconomic signal when predicting income, e.g. married individuals may benefit from dual incomes or household stability.  Never-married or separated individuals might reflect different life stages or economic pressures.

These categories are not ordinal, e.g. widowed or married is not more or less than divorced.

Collapsing sparse categories may improve model stability and interpretability, e.g. married, non-traditional married, previously married, never married.

## Marital Status Group

Marital status group collapses sparse categories into broader groups for model stability and interpretability.

```{r define_marital_group}
# Create marital status group variable

income <- income %>%
  mutate(marital_status_group = case_when(
    marital_status %in% c("Married-civ-spouse", 
                          "Married-AF-spouse") ~ "Married",
    marital_status %in% c("Divorced", 
                          "Separated", 
                          "Married-spouse-absent", 
                          "Widowed") ~ "Previously-married",
    marital_status == "Never-married" ~ "Never-married"))

# Make variable a factor.

income <- income %>%
  mutate(marital_status_group = factor(marital_status_group, 
                                       ordered = FALSE,
                                       levels = c("Never-married", 
                                                  "Previously-married", 
                                                  "Married")))
```

Show distribution of income by marital status group.

```{r}
# Group by marital group.

marital_status_group_prop <- income %>%
  group_by(marital_status_group, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(marital_status_group) %>%
  mutate(prop = count / sum(count))

# Generate plot.

ggplot(marital_status_group_prop, aes(x = marital_status_group, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Proportion of Income Classes by Marital Status Group",
       x = "Marital Status Group",
       y = "Proportion",
       fill = "Income Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

Married individuals show the highest proportion of income earners in the `>50K` category. This suggests that marriage may correlate with financial stability, dual-income households, or career maturity.

Previously-married and Never-married groups lean heavily toward the `<=50K` category, indicating lower income prevalence.

## Hours Per Week

Hours per week represents the number of hours an individual reports working per week in their primary job.

```{r}
hours_per_week_summary <- income %>%
  group_by(hours_per_week) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

ggplot(hours_per_week_summary, aes(x = hours_per_week, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Hours Per Week",
       x = "Hours Per Week",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(hours_per_week_summary$entries) * 1.2)
```

There is a prominent spike at 40 hours, which can be considered a classic full-time benchmark in the US.  It dominates the dataset, reflecting standard employment contracts.  There are smaller peaks at 20, 30, 50 and 60 hours, suggesting common part-time and overtime thresholds.  These are potentially tied to specific industries or roles.  The jagged, irregular tails hint at self-reported data or job-specific norms, e.g. self-employment or gig work.

## Hours Group

The hours group variable is a derived categorical feature that segments individuals based on their reported weekly work hours:

* Underemployed: Works fewer than 30 hours per week,
* Full-time: Works between 30 and 45 hours per week, and
* Overtime: Works more than 45 hours per week.

```{r define_hours_group}
# Create hours group variable.

income <- income %>%
  mutate(hours_group = case_when(
    hours_per_week < 30 ~ "Underemployed",
    hours_per_week >= 30 & hours_per_week <= 45 ~ "Full-time",
    hours_per_week > 45 ~ "Overtime"))

# Make variable a factor.

income <- income %>%
  mutate(hours_group = factor(hours_group,
                              ordered = TRUE,
                              levels = c("Underemployed", 
                                         "Full-time", 
                                         "Overtime")))
```

Show proportion of high earners by hours group.

```{r}
hours_group_prop <- income %>%
  group_by(hours_group, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(hours_group) %>%
  mutate(prop = count / sum(count))
  
ggplot(hours_group_prop, aes(x = hours_group, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  labs(
    title = "Income Proportion by Hours Group",
    x = "Hours Group",
    y = "Proportion",
    fill = "Income Level"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
        plot.title = element_text(size = 14, face = "bold"))
```

The underemployed group is overwhelmingly low income and likely includes part-time, seasonal or precarious workers.  The full-time group is majority `<=$50k` but has a noticeable uptick in high earners.  This suggests that full-time work alone isn't a guarantee of higher income.  The overtime group has the most balanced distribution; it probably includes a mix of skilled labor, self-employed individuals and salaried workers with performance incentives.

## Education

Education is a categorical variable with 16 levels, ranging from Preschool through advanced degrees like Doctorate` and Prof-school.

```{r}
# Make variable a factor.

income <- income %>%
  mutate(education = factor(education, 
                            levels = c("Preschool", 
                                       "1st-4th", 
                                       "5th-6th", 
                                       "7th-8th", 
                                       "9th", 
                                       "10th", 
                                       "11th", 
                                       "12th",
                                       "HS-grad", 
                                       "Some-college", 
                                       "Assoc-voc", 
                                       "Assoc-acdm",
                                       "Bachelors",
                                       "Masters",
                                       "Prof-school", 
                                       "Doctorate")))

education_summary <- income %>%
  group_by(education) %>%
  summarise(entries = n(), .groups = "drop") %>%
  group_by(education) %>%
  mutate(percent = round(entries / sum(entries), 2))

ggplot(education_summary, aes(x = education, y = entries)) +
  geom_col(fill = "steelblue", color = "white") +
  geom_text(aes(label = paste0(scales::comma(entries), "\n", round(percent * 100, 0), "%")),
            vjust = -0.3, size = 3.0) +
  labs(title = "Distribution of Education",
       x = "Education",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(education_summary$entries) * 1.2)
```

High school graduates dominate the sample at 32%; this aligns with national trends.  Some-college and Bachelors categories together make up 40% of the sample, indicating strong representation.  Advanced degrees are relatively rare; combined they account for ~7% of the sample, which could limit model ability to generalize to highly educated groups.  Low education levels are sparse; these are likely to be older adults or immigrants with limited formal schooling.

Plot income proportions by education.

```{r}
# Group by education.

income_education_prop <- income %>%
  group_by(education, income) %>%
  summarise(entries = n(), .groups = "drop") %>%
  group_by(education) %>%
  mutate(prop = entries / sum(entries))

# Plot summary.

ggplot(income_education_prop, 
       aes(x = education, 
           y = prop, 
           fill = income)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Proportion by Education Level",
       x = "Education Level",
       y = "Proportion",
       fill = "Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

There's an unmistakable pattern evident in this chart: the gradient from low education levels to advanced degrees is a textbook example of socioeconomic stratification.  Still its surprising to see how nonlinear the education payoff is in income potential.

The proportion of individuals earning `>50K` doesn’t increase at a constant rate across education levels. Instead, it jumps more sharply at certain thresholds, especially from HS-grad to Bachelors, and again from Masters to Doctorate.

High income really starts to dominate at the Bachelor level and is an inflection point for income potential.  Advanced degrees show the highest income potential, but this comes with investment of time and potentially debt as well.

I'll use ordinal coding to treat education as a ranked factor to capture the income gradient.  I'll create a separate `education_group` variable for this as part of transformations.

## Education Group

Create a separate `education_group` variable.

```{r define_education_level_cat}
# Create education level variable.

income <- income %>%
  mutate(education_group = case_when(
            education %in% c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", 
                                 "10th", "11th", "12th") ~ "Non-HS",
            education %in% c("HS-grad", "Some-college", 
                                 "Assoc-voc", "Assoc-acdm") ~ "HS-some-college",
            education %in% c("Bachelors") ~ "Bachelors",
            education %in% c("Masters") ~ "Masters",
            education %in% c("Prof-school", "Doctorate") ~ "Prof-doctorate"))

# Make variable a factor.

income <- income %>%
  mutate(education_group = factor(education_group,
                                  ordered = TRUE,
                                  levels = c("Non-HS", 
                                             "HS-some-college", 
                                             "Bachelors",
                                             "Masters",
                                             "Prof-doctorate")))
```

Plot proportion of income by education group.

```{r}
# Group by education group.

education_group_prop <- income %>%
  group_by(education_group, income) %>%
  summarise(entries = n(), .groups = "drop") %>%
  group_by(education_group) %>%
  mutate(prop = entries / sum(entries))

# Plot summary.

ggplot(education_group_prop, 
       aes(x = education_group, 
           y = prop, 
           fill = income)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Proportion by Education Level",
       x = "Education Level",
       y = "Proportion",
       fill = "Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

The income jump from Bachelors to Masters to Prof-doctorate is not linear. The gains accelerate, suggesting credential thresholds, e.g., graduate degrees unlocking higher-paying roles, and labor market segmentation, e.g., professional degrees tied to elite occupations.

I will test a natural cubic splines to model this non-linear relationship between your ordinal education variable and the outcome, i.e. `ns(as.numeric(education_group), df = 3)`.

## Capital Gain/Loss

In the UCI Adult Income dataset, the capital gain and capital loss variables don’t represent individual investment transactions like you’d see in a brokerage account. Instead, they are annual amounts reported on tax returns.

Here’s what they represent in practice:

**Capital Gain:** The total taxable profit someone reported in a year from selling assets (stocks, bonds, property, etc.) for more than they paid.

**Capital Loss:** The total deductible loss someone reported in a year from selling assets for less than they paid. Tax rules allow limited reporting of such losses.

In the dataset, most people have zeros for both variables, meaning they didn’t report any gains or losses that year. Nonzero values are relatively rare but signal engagement with investment activity beyond wages.

### Limitation of Capital Gain/Loss as a Predictors

While capital gain and capital loss provide useful signals of investment activity, they capture only realized transactions (profits or losses from assets actually sold). They do not account for asset ownership or wealth holdings that have not been sold.  For example, someone may hold significant investments in real estate or retirement accounts but report zero gains or losses if they did not sell anything that year. 

This means the variables reflect investment activity and non-wage income, not necessarily investment capacity or wealth accumulation.

### Capital Gain

Generate capital gain summary.

```{r}
# Generate summary.

capital_gain_summary <- income %>%
  group_by(capital_gain) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot summary.

ggplot(capital_gain_summary, aes(x = capital_gain, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Capital Gain",
       x = "Capital Gain (Dollars)",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(capital_gain_summary$entries) * 1.2)
```

Capital gains are heavily zero-inflated, and that skew makes it a prime candidate for binary transformation. Most individuals have zero capital gain, so the continuous values only apply to a small subset. Continuous skewed variables can distort coefficients or inflate variance in linear models. A binary flag captures the presence of investment activity, which may correlate with income or occupation.

Plot cumulative distribution of capital gains.

```{r}
# Group by gain.

capital_gain_prop <- capital_gain_summary %>%
  arrange(capital_gain) %>%
  mutate(cumulative = cumsum(entries) / sum(entries))

# Plot proportion.

ggplot(capital_gain_prop, aes(x = capital_gain, y = cumulative)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Cumulative Distribution of Capital Gain",
       x = "Capital Gain (Dollars)",
       y = "Proportion") +
    theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

The cumulative distribution shows that over 92% of individuals report zero or minimal capital gains, with only a small fraction earning substantial amounts (e.g., >10,000).

Capital gain is a strong signal of financial activity and wealth accumulation, but it's concentrated among a small subset of the population.

### Capital Loss

Generate capital loss summary.

```{r}
# Generate summary.

capital_loss_summary <- income %>%
  group_by(capital_loss) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot summary.

ggplot(capital_loss_summary, aes(x = capital_loss, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Capital Loss",
       x = "Capital Loss (Dollars)",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(capital_loss_summary$entries) * 1.2)
```

Capital loss is also heavily zero-inflated.

Plot cumulative distribution of capital losses.

```{r}
# Group by loss.

capital_loss_prop <- capital_loss_summary %>%
  arrange(capital_loss_summary) %>%
  mutate(cumulative = cumsum(entries) / sum(entries))

# Plot proportion.

  ggplot(capital_loss_prop, aes(x = capital_loss, y = cumulative)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Cumulative Distribution of Capital Loss",
       x = "Capital Loss (Dollars)",
       y = "Proportion") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

Over 96% of individuals report zero or minimal capital loss. A steep rise in the cumulative distribution between 1,000 and 2,000 suggests that most reported losses fall within this range. Very few individuals report losses above 2,000.

Combining capital gain and capital loss into a single binary variable reduces two sparse, skewed variables into one binary flag. It preserves the signal that captures whether the individual has any investment-related income or loss. It is also easier to explain and visualize in behavioral segmentation or fairness audits.

## Has Investment Activity

Create a single investment activity flag using capital gain and capital loss.

```{r define_has_investment_activity}
# Create investment activity variable.

income <- income %>%
  mutate(has_investment_activity = if_else(capital_gain > 0 | capital_loss > 0, "Yes", "No"))

# Make variable a factor.

income <- income %>%
  mutate(has_investment_activity = factor(has_investment_activity,
                                          ordered = FALSE,
                                          levels = c("No", "Yes")))
```

Check distribution of the variable.

```{r}
# Group by activity.

investment_by_income <- income %>%
  group_by(has_investment_activity, income) %>%
  summarise(entries = n()) %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot distribution.

ggplot(investment_by_income, 
       aes(x = has_investment_activity, 
           y = percent, 
           fill = income)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Proportion by Investment Activity",
       x = "Investment  Activity",
       y = "Proportion",
       fill = "Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
        plot.title = element_text(size = 14, face = "bold"))
```

Individuals who have not had capital gains or losses are predominantly in the `<=50K` income category.

Those who have reported capital gains or losses show a much higher proportion in the `>50K` category, suggesting that investment behavior correlates with higher earnings.

Investment activity may reflect financial literacy, risk tolerance, or access to disposable income.

It could also signal career stage or education level, since those with more resources and knowledge are more likely to invest.

The plot shows a strong separation in income proportions based on investment behavior. Binary encoding preserves that contrast cleanly.

Save data with features.

```{r save_transformed_data}
saveRDS(income, "../data/income_final.rds")
```

# Correlations

Load transformed income data.

```{r load_transformed_data}
income <- readRDS("../data/income_final.rds")
```

Check for correlations among predictors.

```{r check_correlartions}
# Set vectors.

predictors <- c("career_stage", "marital_status_group", "hours_group",
                "education_group", "has_investment_activity")

ordinal_vars <- c("career_stage", "hours_group", "education_group")

# Run the function to generate correlations summary.

assoc_summary <- test_predictor_associations(income, predictors, ordinal_vars)

rownames(assoc_summary) <- NULL
```

This correlation matrix confirms that the predictors are behaviorally distinct but lightly interrelated, which is ideal for modeling.

Print correlations in table form.

```{r show_correlation_values}
# Print numeric values.

kable(assoc_summary, 
      col.names = c("Pair", "Type", "Value"),
      format.args = list(big.mark = ","),
      align = c("l", "l", "r"))
```

`career_stage ~ marital_status_group`, with a Cramér’s V = 0.406, is the strongest association. This suggests that career progression is meaningfully related to marital status, possibly due to age, stability, or life stage effects. This could also reflect behavioral segmentation: married individuals may cluster in mid-career or peak-earning stages.

Cramér’s V values above 0.3 suggest moderate association between categorical variables. Spearman correlations are also low, indicating weak monotonic relationships between ordinal variables.

# Multicollinearity

Use the custom function `check_multicollinearity_factors` on the income dataset, using the specified predictors. Calculate Variance Inflation Factors (VIFs) to see how much each predictor is correlated with the others.

```{r check_multicollinearity}
predictors <- c("career_stage", "marital_status_group", "hours_group",
                "education_group", "has_investment_activity")

vif_summary <- check_multicollinearity_factors(income, predictors)

rownames(vif_summary) <- NULL

kable(vif_summary, 
      col.names = c("Variable", "VIF"),
      format.args = list(big.mark = ","),
      align = c("l", "r"))
```

All VIFs are below 2.2, which is comfortably within the safe zone (common thresholds are 5 or 10). This means no predictor is linearly dependent on the others, and coefficient estimates should be stable.

This VIF profile confirms that the model should be statistically sound and behaviorally distinct.

# Class Balance

I will use class weights in the training set to counteract the imbalance in the outcome classes (after splitting the income data). 

# Split Data

Split the dataset into 60% training, 20% validation, and 20% test. This allocation provides enough data to train stable models while dedicating a higher-than-usual share to validation and testing. With a large dataset, this approach strengthens model comparison, improves tuning, and ensures that final performance metrics are based on a robust and representative holdout set.

```{r split_data}
set.seed(123)

# Initial train/test split.

train_idx <- createDataPartition(income$income, p = 0.6, list = FALSE)

income_train <- income[train_idx, ]

temp  <- income[-train_idx, ]

# Split remaining into validation/test.

valid_idx <- createDataPartition(temp$income, p = 0.5, list = FALSE)

income_validate <- temp[valid_idx, ]

income_test <- temp[-valid_idx, ]
```

Check class balance in the train, validation and test sets.

```{r check_splits}
# Check balance function.

check_balance <- function(df, name) {
  df %>%
    count(income) %>%
    mutate(prop = round(n / sum(n), 2),
           dataset = name) %>%
    select(dataset,
           income, 
           n,
           prop)
}

# Generate data frame.

check <- bind_rows(check_balance(income_train, "Train"),
                   check_balance(income_validate, "Validation"),
                   check_balance(income_test, "Test"))

# Produce table summary.

kable(check,
      col.names = c("Dataset", "Income Level", "Count", "Percent"),
      caption = "Dataset Class Balance",
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r"))
```

The table confirms the income class split across train, validation, and test sets.

# Apply Class Weights

Use class weighting in the training set to counteract the imbalance in the outcome classes.

```{r apply_weights}
# Calculate the proportion of each class in the training set.

train_props <- prop.table(table(income_train$income_num))

# Assign inverse frequency weights.

income_train$weight <- ifelse(income_train$income_num == 1,
                            1 / train_props["1"],
                            1 / train_props["0"])

# Normalize weights.

income_train$weight <- income_train$weight / mean(income_train$weight)
```

Each row in the training set gets a numeric value in the weight variable such that observations in the minority class `(income_num == 1, typically >50K)`>50K` receive higher weights, and  observations in the majority class `<=50K` receive lower weights.

The weights are normalized to have a mean of 1, so they don’t distort the overall scale of the model’s loss function.

# Train Models

## Linear Regression Indicator Matrix Model

### Prepare Indicator Matrices

Prepare the predictor matrix X, containing numeric representations of age, education, and marital status. The response variable, income, is coded as 0 `<=$50K` or 1 `>$50K`. It is converted to a one-hot (indicator) matrix Y for the multivariate linear regression.

A weight vector establishes class weights for the linear model.

```{r}
# Create indicator matrix for binary response; assumed to be 0/1

G <- income_train$income_num  

# One-hot encoding

Y <- model.matrix(~ factor(G) - 1)

# Class proportions.

class_props <- prop.table(table(G))

# Inverse frequency weights.

weights_vec <- ifelse(G == 1,
                      1 / class_props["1"],
                      1 / class_props["0"])

# Normalize weights.

weights_vec <- weights_vec / mean(weights_vec)
```

### Fit Linear Indicator Model

A multivariate linear regression is fit with the one-hot encoded response matrix. This approach models the probability of each class as a linear combination of predictors.

```{r}
# fit the linear regression model and produce summary.

# model_linear_matrix <- lm(Y ~ poly(career_stage, 2) +
#                               marital_status_group +
#                               hours_group +
#                               splines::ns(as.numeric(education_group), df = 3) +
#                               has_investment_activity, 
#                           data = income_train,
#                           weights = weights_vec)

model_linear_matrix <- lm(Y ~ career_stage * hours_group +
                            education_group * marital_status_group +
                            has_investment_activity, 
                          data = income_train,
                          weights = weights_vec)
```

Earlier model iterations included polynomial and spline terms to capture potential nonlinearity:

* poly(career_stage, 2) modeled career stage as a second-order polynomial, and
# splines::ns(as.numeric(education_group), df = 3) applied a natural spline to education level.

These specifications were later replaced with interaction terms to better capture subgroup structure and improve model interpretability.

### Produce Linear Indicator Model Summary

Produce model summary.

```{r}
summary(model_linear_matrix)
```

The fitted model includes main effects and interactions across career stage, hours worked, education level, marital status, and investment activity. Many coefficients are statistically significant at conventional thresholds, particularly for main effects:

* Career stage (linear and quadratic contrasts),
* Hours worked (linear contrast),
* Education level (linear and quadratic contrasts),
* Marital status (Married), and
* Investment activity.

Several career stage × hours group and education × marital status interactions show significance, suggesting subgroup-specific effects.

However, not all coefficients are statistically significant, especially higher-order contrasts and certain interaction terms. These non-significant estimates may still contribute to model structure or reflect subtle subgroup patterns, but should be interpreted with caution. Their inclusion supports a flexible specification, though future model refinement may consider regularization or pruning for parsimony.

### Check Residuals

#### Residuals Versus Fitted Values

```{r}
#car::residualPlots(model_linear_matrix)

income_train$resid_0 <- residuals(model_linear_matrix)[,1]
income_train$fitted_0 <- fitted(model_linear_matrix)[,1]
income_train$resid_1 <- residuals(model_linear_matrix)[,2]
income_train$fitted_1 <- fitted(model_linear_matrix)[,2]

income_long <- income_train %>%
  select(fitted_0, resid_0, fitted_1, resid_1) %>%
  pivot_longer(
    cols = everything(),
    names_to = c(".value", "class"),
    names_pattern = "(fitted|resid)_(\\d)"
  ) %>%
  mutate(class = paste("Class", class),
         sqrt_abs_resid = sqrt(abs(resid)))


# ggplot(income_train, aes(x = fitted_0, y = resid_0)) +
#   geom_point(alpha = 0.4, color = "steelblue") +
#   geom_smooth(method = "loess", se = FALSE) +
#   labs(title = "Residuals vs Fitted: Class 0", x = "Fitted", y = "Residuals") +
#   theme_minimal()
# 
# ggplot(income_train, aes(x = fitted_1, y = resid_1)) +
#   geom_point(alpha = 0.4, color = "steelblue") +
#   geom_smooth(method = "loess", se = FALSE) +
#   labs(title = "Residuals vs Fitted: Class 1", x = "Fitted", y = "Residuals") +
#   theme_minimal()

ggplot(income_long, aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.4, color = "steelblue") +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~ class) +
  labs(title = "Residuals vs Fitted by Class", x = "Fitted", y = "Residuals") +
  theme_minimal()

```

#### QQ Normal

```{r}
# Create QQ data for each class
qq_data <- list(
  "Class 0" = income_train$resid_0,
  "Class 1" = income_train$resid_1
) %>%
  purrr::imap_dfr(~ {
    qq <- qqnorm(.x, plot.it = FALSE)
    tibble(
      theoretical = qq$x,
      sample = qq$y,
      class = .y
    )
  })

# # Class 0
# ggplot(income_train, aes(sample = resid_0)) +
#   stat_qq() +
#   stat_qq_line(color = "red") +
#   labs(title = "QQ Plot: Class 0", x = "Theoretical Quantiles", y = "Sample Quantiles") +
#   theme_minimal()
# 
# # Class 1
# ggplot(income_train, aes(sample = resid_1)) +
#   stat_qq() +
#   stat_qq_line(color = "red") +
#   labs(title = "QQ Plot: Class 1", x = "Theoretical Quantiles", y = "Sample Quantiles") +
#   theme_minimal()

ggplot(qq_data, aes(sample = sample)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  facet_wrap(~ class) +
  labs(title = "QQ Plot by Class",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

Both QQ plots for both classes look reasonable and support the assumption of approximate normality. The points closely follow the red reference line, especially through the central quantiles, which suggests that the residuals are symmetrically distributed and not heavily skewed. There are mild deviations at the tails, but nothing egregious.

In context, this normality suggests that the residuals aren’t misspecified, even if the scale-location plots reveal heteroscedasticity (see below). The residuals may have non-constant variance, but their distribution is still roughly normal.

#### Scale Location

```{r}
# Class 0
# ggplot(income_train, aes(x = fitted_0, y = sqrt(abs(resid_0)))) +
#   geom_point(alpha = 0.4, color = "steelblue") +
#   geom_smooth(method = "loess", se = FALSE, color = "darkblue") +
#   labs(title = "Scale-Location Plot: Class 0", x = "Fitted Values", y = "√|Residuals|") +
#   theme_minimal()
# 
# # Class 1
# ggplot(income_train, aes(x = fitted_1, y = sqrt(abs(resid_1)))) +
#   geom_point(alpha = 0.4, color = "steelblue") +
#   geom_smooth(method = "loess", se = FALSE, color = "darkblue") +
#   labs(title = "Scale-Location Plot: Class 1", x = "Fitted Values", y = "√|Residuals|") +
#   theme_minimal()

ggplot(income_long, aes(x = fitted, y = sqrt_abs_resid)) +
  geom_point(alpha = 0.4, color = "steelblue") +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~ class) +
  labs(title = "Scale-Location Plot by Class",
       x = "Fitted Values",
       y = expression(sqrt("|Residuals|"))) +
  theme_minimal()

```

The unbounded residuals are a direct consequence of using a linear model on an indicator matrix for classification. Since the model isn't constrained to output probabilities between 0 and 1, it can produce fitted values (and thus residuals) that extend well beyond that range.

The curved shape of the scale-location plots indicates heteroscedasticity, with residual variance increasing at the extremes of fitted values — a sign that the model is less stable when it’s most confident. This fan-like spread suggests that prediction error grows with predicted probability, potentially due to class imbalance or unmodeled nonlinear effects. The overall symmetry of the residuals is reassuring, but any asymmetry or tail heaviness would point to bias or structural misfit, reinforcing the need for a model that better captures bounded probabilities and variance behavior.

#### Leverage and Influential Points

```{r}
# Extract design matrix and leverage.

X <- model.matrix(model_linear_matrix)

H <- X %*% solve(t(X) %*% X) %*% t(X)

leverage_vec <- diag(H)

p <- ncol(X)

# Get residuals matrix.

resid_mat <- residuals(model_linear_matrix)  # n x k

sigma_sq_vec <- apply(resid_mat, 2, function(r) mean(r^2))  # per class

# Compute Cook's distance per class.

cooks_df <- as_tibble(resid_mat) %>%
  mutate(index = row_number()) %>%
  pivot_longer(-index, names_to = "class", values_to = "resid") %>%
  mutate(leverage = rep(leverage_vec, times = ncol(resid_mat)),
         sigma_sq = rep(sigma_sq_vec, each = nrow(resid_mat)),
         cooks = (resid^2 / (p * sigma_sq)) * (leverage / (1 - leverage)^2))

# Plot residuals vs. leverage, colored by Cook's distance.

ggplot(cooks_df, aes(x = leverage, y = resid, color = cooks)) +
  geom_point(alpha = 0.6) +
  scale_color_gradient(low = "gray70", high = "red") +
  geom_smooth(method = "loess", se = FALSE, color = "blue", linetype = "dotted") +
  facet_wrap(~ class) +
  labs(title = "Residuals vs. Leverage by Class",
       subtitle = "Color intensity reflects Cook's Distance",
       x = "Leverage",
       y = "Residual",
       color = "Cook's D") +
  theme_minimal()
```

I assessed model influence using leverage and Cook’s distance diagnostics. Most observations showed low to moderate leverage, with no class-specific imbalance. A few cases combined high leverage and large residuals, indicating potential influence, but these were evenly distributed across income groups. Residual patterns were stable, supporting the model’s fairness and structural integrity.

### Predict Class Scores

Predict class scores initially with training data.

```{r}
# Predict class scores.

train_pred <- predict(model_linear_matrix, newdata = income_train)
```

For each observation, assign the class with the highest predicted score. We then recode it back to match the original binary labels (0 or 1).

```{r}
# Assign predicted class (1 or 2).

train_class_pred <- max.col(train_pred)

# Recode predicted class to match binary response (0/1).

train_class_pred_binary <- ifelse(train_class_pred == 1, 0, 1)
```

### Generate Confusion Matrix

Evaluate the predicted classes against the true labels using a confusion matrix.

```{r}
# Evaluate classification performance.

cm_linear_matrix_train <- caret::confusionMatrix(factor(train_class_pred_binary), factor(income_train$income_num))

cm_linear_matrix_train
```

The model achieves an overall accuracy of 77.7% (95% CI: 77.1–78.2%), which is statistically higher than the no-information baseline of 75.9% (p < 0.001). While this improvement is significant given the large sample size, the absolute gain in accuracy is modest.

Performance is stronger at identifying the majority group (class 0, <$50K), with sensitivity of 75.2% and a very high positive predictive value of 94.2%. Specificity is also strong at 85.4%, but the negative predictive value is lower (52.2%), meaning the model struggles more with correctly identifying higher-income cases. The balanced accuracy is 80.3%, and Cohen’s Kappa of 0.50 indicates moderate agreement beyond chance.

### Extreme Probabilities

Although we can produce predicted classes and metrics, the predicted probabilities from a linear model are not constrained to 0-1. This can result in nonsensical probabilities, motivating the use of logistic regression for binary outcomes.

The table below summarizes the number and percentage of predicted probabilities from the linear regression model that fall outside the valid 0–1 range for each class. As expected, linear regression applied to a binary outcome can produce estimates below 0 or above 1, highlighting a limitation of this approach for classification tasks.

First, we extract the predicted probabilities for each class from the linear regression model. These probabilities represent the model’s estimated likelihood that each individual falls into the ≤$50K or >$50K income category.

```{r}
# Extract columns.

prob_under50k <- train_pred[, "factor(G)0"]

prob_over50k  <- train_pred[, "factor(G)1"]
```

To assess the appropriateness of linear regression for a binary outcome, we identify predictions that fall outside the valid probability range of 0 to 1. The table below shows the number and percentage of such predictions for each class.

```{r}
# Count out-of-bounds for each class.

out_under <- sum(prob_under50k < 0 | prob_under50k > 1)

out_over  <- sum(prob_over50k  < 0 | prob_over50k  > 1)

total <- nrow(train_pred)

# Summary table.

check_summary <- data.frame(Cclass = c("<=50K", ">50K"),
                            out_of_bounds = c(out_under, out_over),
                            total = total,
                            percent_out_of_bounds = round(100 * c(out_under, out_over) / total, 2))

kable(check_summary,
      col.names = c("Class", "Out of Bounds", "Total", "Percent Out of Bounds"),
      caption = "Out of Bounds Data for Each Class",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r"))
```

The plot below illustrates the distribution of predicted probabilities for both income classes. The dashed red lines mark the valid 0–1 probability range. Any predictions beyond these boundaries are not interpretable as probabilities, demonstrating why logistic regression is generally preferred for binary classification problems.

```{r}
# Convert to long format.

df_long <- as.data.frame(train_pred) %>%
  pivot_longer(cols = everything(), 
               names_to = "Class", 
               values_to = "Probability") %>%
  mutate(Class = if_else(Class == "factor(G)0", "<=50K", ">50K"))

# Plot.

ggplot(df_long, aes(x = Probability, fill = Class)) +
  geom_histogram(bins = 50, color = "white", position = "dodge") +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = 1, color = "red") +
  geom_vline(xintercept = 1, linetype = "dashed", linewidth = 1, color = "red") +
  labs(
    x = "Predicted Probability",
    y = "Count",
    title = "Distribution of Linear Regression Predicted Probabilities by Class"
  ) +
  scale_fill_manual(values = c("<=50K" = "steelblue", ">50K" = "lightsteelblue")) +
  theme_minimal()
```

Together, these outputs provide a numeric indication and clear visual of the constraints of using linear regression for a categorical outcome, setting the stage for comparison with the logistic regression model.

Because linear regression is not bounded, the model produces predicted values below 0 and above 1, which are not valid probabilities. This creates problems for interpretation, since values like –0.2 or 1.3 cannot be meaningfully explained as likelihoods. It also complicates classification, as thresholding these outputs can distort decision rules, and the predictions themselves are not well calibrated to reflect true probabilities.

### Generate ROC Curve and AUC Metric

To evaluate the discriminatory power of the indicator regression model, extract the predicted probabilities for the positive class and used them as inputs to generate a Receiver Operating Characteristic (ROC) curve.

```{r}
# Column 2 corresponds to class 1 (income_num == 1).

score_class1 <- train_pred[, 2]

roc_obj_lm <- pROC::roc(response = income_train$income_num, predictor = score_class1)

# Plot ROC curve.

plot(roc_obj_lm, col = "steelblue", lwd = 2, main = "ROC Curve for Indicator Regression")
```

The blue line bows confidently toward the top-left corner, which indicates high sensitivity and specificity across thresholds. The shape suggests an AUC in the high 0.7s to low 0.8s.

The gray line represents random guessing. The model clearly outperforms this baseline, which confirms meaningful signal in the predictors.

```{r}
# AUC value.

pROC::auc(roc_obj_lm)
```

This AUC score indicates that the model can distinguish between `>50K` and `<=50K` earners with 88.4% accuracy across all thresholds.

## Logistic Regression

### Fit Logistic Regression Model

Build a logistic regression model to estimate income likelihood, using career stage, marital status, hours worked, education, and investment activity as predictors, with adjustments for weighting.

```{r}
# Produce logistic model.
# 
# model_logistic <- glm(income_num ~ 
#                         poly(career_stage, 2) +
#                         marital_status_group +
#                         hours_group +
#                         splines::ns(as.numeric(education_group), df = 4) +
#                         has_investment_activity,
#                     data = income_train, weights = weight, family = "binomial")

model_logistic <- glm(income ~ career_stage * hours_group +
                        education_group * marital_status_group +
                        has_investment_activity,
                      data = income_train, weights = weight, family = "binomial")
```

Similarly to the linear indicator model, earlier model iterations included polynomial and spline terms to capture potential nonlinearity:

* poly(career_stage, 2) modeled career stage as a second-order polynomial, and
# splines::ns(as.numeric(education_group), df = 3) applied a natural spline to education level.

These specifications were later replaced with interaction terms to better capture subgroup structure and improve model interpretability.

Note on the non-integer warning message.  This message is triggered when using non-integer weights in a binomial glm(). The binomial family expects counts of successes and failures, and when weights are fractional, it assumes we are modeling aggregated binomial outcomes, which is not the case for this model.

Based on my research, the warning is safe to ignore because I am using weights to adjust for class imbalance, not to model grouped binomial trials.  The response variable is binary and the weights are just importance weights, not trial counts.

The model still fits correctly and returns valid coefficients, standard errors, and predictions.

### Produce Logistic Regression Model Summary

Produce model summary.

```{r}
summary(model_logistic)
```

The logistic regression model estimates the log-odds of earning above the income threshold, incorporating main effects and interactions across career stage, hours worked, education level, marital status, and investment activity.

Significant main effects include:

* Career stage (linear and quadratic contrasts),
* Hours worked (both contrasts),
* Education level (strong linear effect and 4th-degree contrast),
* Marital status (Married), and
* Investment activity (Yes).

Notable interactions include `career_stage.Q × hours_group.L` and `education_group.C × marital_status_groupPreviously-married`; both show statistical significance, suggesting nuanced subgroup effects.

Non-significant terms include several higher-order education contrasts and interaction terms.  These may reflect over-specification or weak subgroup differentiation.

Despite some non-significant coefficients, the model achieves strong overall fit:

* Residual deviance: 16,310 (vs. null deviance 27,084),
* AIC: 20,498, and
* Significance of overall model: p < 2e-16.

### Check Residuals

```{r}
car::residualPlots(model_logistic)
```

Residual plots for the logistic regression model indicate a well-calibrated fit across most predictor groups. The Pearson residuals vs. linear predictor plot shows the expected funnel shape, with no major curvature or systematic deviation, suggesting that the logit link function is appropriate.

Box plots of residuals across categorical variables reveal generally symmetric distributions, though mild skew and spread differences in `education_group` and `career_stage` suggest potential subgroup heterogeneity. These patterns are far less pronounced than in the linear model, reinforcing the logistic model’s suitability for binary classification.

Overall, the residual diagnostics support the logistic regression model as a more interpretable and statistically coherent alternative to the linear matrix model.

### Predict Class Scores

Extract the predicted probabilities for the positive class.

```{r}
income_train$predicted_prob <- predict(model_logistic, type = "response")
```

### Generate Confusion Matrix

Evaluate the predicted classes against the true labels using a confusion matrix.

```{r}
threshold <- 0.5

income_train$predicted_class <- ifelse(income_train$predicted_prob > 0.5, 1, 0)

cm_logistic_train <- caret::confusionMatrix(factor(income_train$predicted_class),
                                            factor(income_train$income_num))

cm_logistic_train
```

The logistic regression model demonstrates solid classification performance, correctly identifying the majority of cases with an overall accuracy of 77.6%. This performance was statistically superior to the baseline no-information rate of 75.9%, with a p-value of 1.51e-08, indicating that the model adds meaningful predictive value beyond chance.

From a class-specific perspective, the model was particularly strong in identifying high-income individuals (class 0), achieving a sensitivity of 74.8% and a positive predictive value of 94.5%. This means that when the model predicted someone to be in the high-income group, it was correct nearly 95% of the time. Conversely, its ability to detect low-income individuals (class 1) was more limited, with a negative predictive value of 52.1%, suggesting that predictions for this group were less reliable.

The specificity, or the model’s ability to correctly identify low-income cases, was 86.3%, which balances the sensitivity and contributes to a balanced accuracy of 80.6%. The Kappa statistic, measuring agreement beyond chance, was 0.50, indicating moderate concordance between predicted and actual classifications.

Finally, the McNemar’s test yielded a highly significant result (p < 2.2e-16), suggesting asymmetry in the types of misclassifications made — a signal that the model may favor one class over the other, which could have implications for fairness or downstream decision-making.

### Generate ROC Curve and AUC Metric

Generate a Receiver Operating Characteristic (ROC) curve.

```{r}
roc_obj_log <- pROC::roc(income_train$income_num, income_train$predicted_prob)

plot(roc_obj_log, col = "steelblue", main = "ROC Curve for Logistic Regression")
```

This blue line bows toward the top-left corner, which indicates high sensitivity and specificity across thresholds. The shape suggests an AUC in the high 0.7s to low 0.8s, which is similar to the linear indicator model.

The model clearly outperforms this baseline, which confirms meaningful signal in the predictors.

```{r}
pROC::auc(roc_obj_log)
```

This AUC score indicates that the model can distinguish between `>50K` and `<=50K` earners with 88.6% accuracy across all thresholds, which is slightly better than the lineear indicator model.

# Compare Models

## Training Data

### Confusion Matrix Comparison

A side-by-side confusion matrices reveal how linear and logistic models differ in classification behavior.

```{r}
# Extract confusion matrix tables.

cm_lm <- cm_linear_matrix_train$table

cm_lg <- cm_logistic_train$table

# Convert to data frames and add model labels.

df_linear <- as.data.frame(cm_lm)

df_linear$model <- "Linear"

df_logistic <- as.data.frame(cm_lg)

df_logistic$model <- "Logistic"

# Combine both into one tidy data frame.

df_combined <- rbind(df_linear, df_logistic)

# Rename columns for clarity.

colnames(df_combined) <- c("Predicted", "Actual", "Count", "Model")

# Add TP/FP/TN/FN label.

df_combined$Label <- with(df_combined, ifelse(
  
  Predicted == 1 & Actual == 1, "TP",
  
  ifelse(Predicted == 1 & Actual == 0, "FP",
         
  ifelse(Predicted == 0 & Actual == 0, "TN",
         
  "FN"))))

# Reorder columns for clarity.

df_final <- df_combined[, c("Model", "Predicted", "Actual", "Label", "Count")]

# View result.

kable(df_final,
      col.names = c("Model", "Predicted", "Actual", "Label", "Count"),
      caption = "Confusion Matrix Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r", "r"))
```

Both the linear and logistic models demonstrate strong performance in identifying high-income individuals, each correctly classifying approximately 4,000 true positives. However, subtle differences in their error patterns reveal distinct strengths:

The logistic model made fewer false positives, meaning it was more conservative in predicting high-income status. This translates to higher specificity, which is valuable when the cost of misclassifying low-income individuals as high-income is high — for example, in resource allocation or eligibility screening.

The linear model, on the other hand, captured slightly more true positives, showing a modest edge in sensitivity. This makes it better suited for contexts where missing a truly eligible individual carries greater risk — such as determining access to financial support or benefit programs.

In short, the linear model leans toward inclusivity, minimizing missed opportunities, while the logistic model favors precision, reducing overextension. The choice between them depends on the operational priorities: whether the goal is to maximize coverage or minimize misclassification.

### Performance Metric Comparison

To assess model performance beyond raw classification counts, we compare key evaluation metrics from linear and logistic regression, including accuracy, sensitivity, specificity, and predictive values.

```{r}
# Extract metrics from both slots.

overall_linear <- cm_linear_matrix_train$overall

byclass_linear <- cm_linear_matrix_train$byClass

overall_logistic <- cm_logistic_train$overall

byclass_logistic <- cm_logistic_train$byClass

# Select key metrics.

metrics_overall <- c("Accuracy", "Kappa")

metrics_byclass <- c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Balanced Accuracy")

# Build data frame.

df_metrics <- data.frame(metric = c(metrics_overall, 
                                    metrics_byclass),
                         linear_train = round(c(overall_linear[metrics_overall], 
                                          byclass_linear[metrics_byclass]), 4),
                         logistic_train = round(c(overall_logistic[metrics_overall], 
                                            byclass_logistic[metrics_byclass]), 4),
                         row.names = NULL)

# View result.

kable(df_metrics,
      col.names = c("Metric", "Linear", "Logistic"),
      caption = "Model Metric Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r"))
```

Both the linear and logistic models exhibit nearly identical overall accuracy, hovering around 77.6%, with Kappa values just under 0.50, indicating moderate agreement beyond chance. However, subtle differences in their diagnostic metrics reveal distinct tendencies:

The linear model shows slightly higher sensitivity (75.2%), meaning it’s marginally better at identifying true high-income cases.

The logistic model edges ahead in specificity (86.3%), making it more effective at correctly identifying low-income individuals.

Precision for high-income predictions is excellent in both models, with positive predictive values above 94%.

Negative predictive values are modest and nearly identical (~52%), reflecting the challenge of reliably identifying low-income cases.

Balanced accuracy, which averages sensitivity and specificity, is slightly higher for the logistic model (80.6% vs. 80.3%).

### AUC Comparison (Versus Accuracy and Kappa)

AUC measures how well a model ranks positives above negatives across all thresholds. It’s threshold-agnostic and robust to class imbalance. Accuracy is threshold-dependent and can be misleading if one class dominates. Kappa adjusts accuracy for chance agreement, but still depends on a fixed threshold.

AUC tells us how well the models discriminate, not just how often it’s “right” at a single cutoff. This is especially useful when comparing models with similar accuracy but different ranking behavior.

```{r}
# Extract AUCs.

auc_lm   <- pROC::auc(roc_obj_lm)

auc_log  <- pROC::auc(roc_obj_log)

# Create comparison data frame.

auc_comparison <- data.frame(Model = c("Linear", "Logistic"),
                             AUC = c(auc_lm, auc_log))

# View result.

kable(auc_comparison,
      col.names = c("Model", "AUC"),
      caption = "AUC Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r"))
```

These are nearly identical, suggesting both models rank cases similarly, and the logistic model has a slightly higher value.

At 0.88, these models have very good discrimination, separating high-income from low-income individuals across thresholds.

## Validation Data

### Linear Regression with an Indicator Matrix

#### Predict Class Scores

```{r}
# Predict class scores.

pred_valid <- predict(model_linear_matrix, newdata = income_validate, type = "response")
```

For each observation, assign the class with the highest predicted score. We then recode it back to match the original binary labels (0 or 1).

```{r}
# Assign predicted class (1 or 2).

class_pred_valid <- max.col(pred_valid)

# Recode predicted class to match binary response (0/1).

class_pred_binary_valid <- ifelse(class_pred_valid == 1, 0, 1)
```

#### Generate Confusion Matrix

Evaluate the predicted classes against the true labels using a confusion matrix.

```{r}
# Evaluate classification performance.

cm_linear_matrix_validate <- caret::confusionMatrix(factor(class_pred_binary_valid),
                                                    factor(income_validate$income_num))

cm_linear_matrix_validate
```

On the validation set, the linear model achieved 78.4% accuracy and a Kappa of 0.51, indicating moderate agreement beyond chance. It showed strong sensitivity (76.1%) and specificity (85.5%), with a balanced accuracy of 80.8%. High-income predictions were highly precise (94.3% PPV), while low-income predictions remained less reliable (53.2% NPV). Overall, the model generalizes well and maintains strong class balance.

### Logistic Regression

#### Predict Class Scores

Predict probabilities and class labels.

```{r}
# Predict probabilities for class 1 (income >50K).

pred_probs_logistic <- predict(model_logistic, newdata = income_validate, type = "response")

# Convert to binary class predictions using threshold 0.5.

class_pred_logistic <- ifelse(pred_probs_logistic >= 0.5, 1, 0)

```

#### Generate Confusion Matrix

Evaluate with confusion matrix.

```{r}

cm_logistic_validate <- caret::confusionMatrix(factor(class_pred_logistic),
                                            factor(income_validate$income_num))

cm_logistic_validate
```

The logistic model achieved 78.3% accuracy on the validation set, with a Kappa of 0.51, indicating moderate agreement beyond chance. It showed strong specificity (86.3%) and solid sensitivity (75.7%), resulting in a balanced accuracy of 81.0%. Predictions for high-income cases were highly precise (94.6% PPV), while low-income predictions remained less reliable (53.0% NPV). Overall, the model maintains strong class separation and generalizes well.

### Compare Metrics From Validation Data

Add metrics to metrics data frame.

```{r}
# Extract metrics from both slots.

overall_linear <- cm_linear_matrix_validate$overall

byclass_linear <- cm_linear_matrix_validate$byClass

overall_logistic <- cm_logistic_validate$overall

byclass_logistic <- cm_logistic_validate$byClass

# Select key metrics.

metrics_overall <- c("Accuracy", "Kappa")

metrics_byclass <- c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Balanced Accuracy")

# Build data frame.

df_metrics_valid <- data.frame(
                         linear_valid = round(c(overall_linear[metrics_overall], 
                                          byclass_linear[metrics_byclass]), 4),
                         logistic_valid = round(c(overall_logistic[metrics_overall], 
                                            byclass_logistic[metrics_byclass]), 4),
                         row.names = NULL)

df_metrics <- cbind(df_metrics, df_metrics_valid) %>%
  select(metric,
         linear_train,
         logistic_train,
         linear_valid,
         logistic_valid)

# View result.

kable(df_metrics,
      col.names = c("Metric", "Linear Train", "Logistic Train", "Linear Validate", "Logistic Validate"),
      caption = "Model Metric Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r", "r"))
```

Across both training and validation sets, the linear and logistic models performed nearly identically, with overall accuracy consistently around 77–78% and Kappa values just above 0.50, indicating moderate agreement beyond chance.

The linear model showed a slight edge in sensitivity, meaning it was marginally better at identifying true high-income cases. In contrast, the logistic model consistently outperformed in specificity, making it more effective at correctly identifying low-income individuals. This tradeoff held across both data splits.

Both models maintained excellent precision for high-income predictions, with positive predictive values exceeding 94%, while predictions for low-income cases remained less reliable, with negative predictive values around 53%. Balanced accuracy was slightly higher for the logistic model in both training and validation, suggesting a modest advantage in class equity.

In summary, the linear model leans toward inclusivity, favoring true positive detection, while the logistic model favors precision, reducing false positives. This consistent tradeoff across datasets provides a clear lens for selecting the model that best aligns with your operational priorities.

# Choose Final Model

Given comparable performance across all key metrics, I selected the logistic regression model as the final classifier. It offers strong sensitivity and specificity, excellent precision for high-income predictions, and slightly better balanced accuracy. Unlike the linear indicator matrix approach, logistic regression avoids calibration issues and provides a more principled framework for binary classification, making it better suited for deployment and stakeholder interpretation.

## Retrain Final Model

Retrain the final logistic regression model on the full training and validation data, then evaluate on the test set once. This gives the cleanest estimate of how the model will perform in deployment.

Combine train and validation sets.

```{r}
income_train <- income_train %>%
  select(-weight,
         -predicted_prob,
         -predicted_class)

income_full <- rbind(income_train, income_validate)
```

Calculate class weights for the final model.

```{r}
# Calculate the proportion of each class in the training set.

full_props <- prop.table(table(income_full$income_num))

# Assign invserse frequency weights.

income_full$weight <- ifelse(income_full$income_num == 1,
                            1 / full_props["1"],
                            1 / full_props["0"])

# Normalize weights.

income_full$weight <- income_full$weight / mean(income_full$weight)
```

Produce the final model.

```{r}
# Produce logistic model.

# model_logistic_final <- glm(income_num ~ 
#                           poly(career_stage, 2) +
#                           marital_status_group +
#                           hours_group +
#                           splines::ns(as.numeric(education_group), df = 3) +
#                           has_investment_activity,
#                     data = income_full, weights = weight, family = "binomial")

model_logistic_final <- glm(income ~ career_stage * hours_group +
                              education_group * marital_status_group +
                              has_investment_activity,
                            data = income_full, weights = weight, family = "binomial")

summary(model_logistic_final)
```

There is an increase in deviance and AIC compared to the prior logistic model.  This reflects the larger sample size, not a drop in model quality. The final model is trained on more data, which naturally increases total deviance but improves generalizability.

Coefficients remain stable across models, with minor shifts reflecting the added data. The career stage terms show stronger curvature, and the marital and hours effects slightly intensify, suggesting more robust estimates.

The final model preserves the structure and significance of the original while benefiting from a larger, more diverse training set. Coefficient magnitudes are consistent, and all predictors remain highly significant. The retrained model is better equipped to generalize, with more stable estimates and improved reliability for downstream evaluation.

## Evaluate Model

Run prediction using the final model with the test set.

```{r}
# Predict probabilities for class 1 (income >50K).

pred_probs_logistic_final <- predict(model_logistic_final, newdata = income_test, type = "response")

# Convert to binary class predictions using threshold 0.5.

class_pred_logistic_final <- ifelse(pred_probs_logistic_final >= 0.5, 1, 0)
```

Generate the confusion matrix for the final test set.

```{r}
cm_logistic_test <- caret::confusionMatrix(factor(class_pred_logistic_final),
                                            factor(income_test$income_num))

cm_logistic_test
```

On the test set, the final logistic regression model achieved 77.7% accuracy with a Kappa of 0.50, indicating moderate agreement beyond chance. It maintained strong specificity (86.1%) and solid sensitivity (74.9%), resulting in a balanced accuracy of 80.6%. Predictions for high-income cases were highly precise (94.5% PPV), while predictions for low-income cases remained less reliable (52.2% NPV). These results confirm that the model generalizes well and preserves its class performance tradeoffs in unseen data.

Add metrics to data frame.

```{r}
# Extract metrics from both slots.

overall_logistic <- cm_logistic_test$overall

byclass_logistic <- cm_logistic_test$byClass

# Select key metrics.

metrics_overall <- c("Accuracy", "Kappa")

metrics_byclass <- c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Balanced Accuracy")

# Build data frame.

df_metrics_test <- data.frame(
                         logistic_test = round(c(overall_logistic[metrics_overall], 
                                            byclass_logistic[metrics_byclass]), 4),
                         row.names = NULL)

df_metrics <- cbind(df_metrics, df_metrics_test) %>%
  select(metric,
         linear_train,
         logistic_train,
         linear_valid,
         logistic_valid,
         logistic_test)

# View result.

kable(df_metrics,
      col.names = c("Metric", 
                    "Linear Train", 
                    "Logistic Train", 
                    "Linear Validate", 
                    "Logistic Validate", 
                    "Logistic Test"),
      caption = "Model Metric Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r", "r", "r"))
```

Compared to earlier models, logistic regression consistently matches or slightly outperforms linear regression across key metrics. It shows marginal gains in specificity and balanced accuracy, particularly on the validation set, suggesting better discrimination between income classes. The model’s performance is remarkably consistent across train, validation, and test sets, with minimal drift in accuracy, Kappa, or class-wise metrics—underscoring its robustness and suitability for deployment.

Overall, the final model balances interpretability, fairness, and predictive strength, making it a defensible choice for reporting and stakeholder communication.

# Final Analysis

## Conclusions

This analysis compared linear regression (via indicator matrix encoding) and logistic regression for classifying individuals as earning above or below $50,000 annually, using key socioeconomic predictors from the UCI Adult Income dataset.

Across training, validation, and test sets, logistic regression consistently outperformed linear regression in accuracy, sensitivity, Kappa, and negative predictive value. Both models achieved high specificity and precision for the majority class, but logistic regression demonstrated better balance and generalization—particularly in identifying minority-class individuals.

Logistic regression’s bounded probability outputs and alignment with binary classification theory made it the more interpretable and appropriate choice. In contrast, the linear model, while competitive on some metrics, was hindered by unbounded predictions and thresholding limitations.

Given its stronger predictive reliability, theoretical coherence, and clearer interpretability, logistic regression was selected as the final model. It offers robust insight into how career stage, marital status, working hours, education, and investment activity relate to income classification—and provides a transparent, reproducible foundation for downstream reporting and fairness auditing.

## Limitations of Linear Regression as a Classifier

While linear regression can be repurposed for binary classification by thresholding predicted values, it suffers from several critical limitations:

**Unbounded predictions:** Unlike logistic regression, linear regression outputs are not constrained to 0-1, making probability interpretation invalid and thresholding arbitrary.

**Violation of assumptions:** The error distribution and homoscedasticity assumptions of linear regression do not hold in classification tasks, leading to biased estimates and unreliable inference.

**Poor calibration:** Predicted values do not reflect true class probabilities, which undermines decision-making in threshold-sensitive applications.

**No natural log-odds interpretation:** Coefficients lack the intuitive odds-based interpretation that logistic regression provides, complicating stakeholder communication.

These limitations motivated a shift toward logistic regression, which offers bounded, interpretable outputs and better alignment with classification goals.

## Challenges and Solutions

| Challenge | Solution | 
| --------- | -------- |
| Linear regression produced unbounded predictions not interpretable as probabilities |	Quantified the extent of unbounded outputs and explicitly acknowledged the theoretical misalignment with binary classification. Used this to illustrate the limitations of general-purpose models for classification tasks. |
| Class imbalance skewed sensitivity and specificity | Applied class weights during model fitting to counteract imbalance and improve recall for underrepresented income group. Supplemented with full confusion matrix metrics (e.g., kappa, balanced accuracy) to assess fairness and performance. |
| Ensuring consistent encoding across models | Applied indicator matrix encoding uniformly to preserve comparability and reproducibility. Verified factor alignment across splits. |
| Translating nuanced metric tradeoffs for stakeholders |	Crafted concise narratives emphasizing practical relevance (e.g., “Logistic regression better identifies low-income individuals without sacrificing overall accuracy”). |
| Balancing interpretability with predictive performance | Selected logistic regression for its theoretical soundness, bounded outputs, and clearer coefficient-based insights into socioeconomic drivers of income. |


























