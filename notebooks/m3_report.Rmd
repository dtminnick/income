---
title: "Predicting Income Level from Demographic and Behavioral Variables"
subtitle: "A Comparison of Linear Regression with an Indicator Matrix and Logistic
  Regression Approaches"
author: "Donnie Minnick, Statistical Learning - Fall A 2025"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  word_document: default
  html_document:
    df_print: paged
    theme:
      version: 4
      bootswatch: flatly
      primary: '#2C3E50'
      secondary: '#18BC9C'
      base_font: Arial
      heading_font: Calibri
---

# Project Overview

This project evaluates two modeling approaches, linear regression with an indicator matrix and logistic regression, to determine which more reliably classifies individuals as earning above or below $50,000 per year. Using the UCI Adult Income dataset, the comparison focuses on career stage, marital status, working hours, education level, and investment activity as explanatory variables.

I compare model performance using practical measures, including accuracy, sensitivity, specificity, and Area Under the ROC Curve (AUC). 

My goal is to answer the question: how do linear regression with an indicator matrix and logistic regression compare in their ability to classify individuals as earning above or below $50,000 per year, based on career stage, marital status, working hours, education level, and investment activity and other demographic and behavioral variables, in terms of accuracy, sensitivity, specificity, and AUC?

## Data Suitability

The UCI Adult Income dataset is well-suited for classification, with a binary target (`<=50K` vs. `>50K`) and a rich mix of predictors:

**Career Stage & Education:** Age, occupation, work experience, and education (years and attainment) proxy human capital and labor market position.

**Financial Behavior:** Capital gains/losses signal investment activity beyond earned income.

**Socioeconomic Context:** Marital status and hours worked reflect broader income dynamics.

**Mixed Data Types:** Categorical and continuous features enable evaluation of model handling across variable types.

**Relevance:** These variables capture key drivers of upward mobility—education, career, and financial engagement.

## Candidate Models

### Linear Regression with an Indicator Matrix

Used here as a benchmark, linear regression estimates class probabilities from a binary indicator response. While it reveals linear relationships (e.g., age, hours worked), it can produce values outside the 0–1 range and lacks a probabilistic foundation.

### Logistic Regression

A standard for binary classification, logistic regression models log-odds as a linear function of predictors. It constrains outputs to 0-1 and yields interpretable odds ratios, making it well-suited for this task.

### Rationale

Comparing these models on accuracy, sensitivity, specificity, and AUC highlights trade-offs between a general-purpose regression and a classification-specific approach.

### Baseline Expectations

Before modeling, it's useful to anticipate how each approach may perform:

* **Linear Regression:** As a benchmark, it may yield reasonable accuracy but often produces out-of-bound probabilities and weak sensitivity/specificity due to its misalignment with classification thresholds.

* **Logistic Regression:** Purpose-built for binary outcomes, it offers better calibration, constrained predictions, and interpretable odds ratios—making it more reliable for this task.

* **Comparative Outlook:** Logistic regression is expected to outperform on AUC and balanced metrics, while linear regression may expose the limitations of general-purpose models in classification contexts.

## Modeling Outcome Preview

This section summarizes the final model selection, key performance metrics, and major challenges addressed throughout the pipeline.  See **Choose Final Model** and **Final Analysis** sections for details.

### Final Model Choice

Logistic regression was selected as the final model after benchmarking against a linear indicator approach. It consistently delivered stronger predictive balance, clearer interpretability, and bounded probability outputs. On the test set, it achieved a balanced accuracy of 0.73 and demonstrated reliable performance across income subgroups.

### Key Performance Metrics

* **Balanced Accuracy:** 0.73 on unseen test set

* **Sensitivity / Specificity:** High sensitivity with improved specificity for minority class

* **Residual Diagnostics:** Logistic regression showed no systematic bias or variance inflation. Linear regression, by contrast, exhibited heteroscedasticity and residual asymmetry, limiting reliability.

### Limitations of Linear Regression as a Classifier, Challenges and Solutions 

Initial modeling with linear regression revealed critical limitations for binary classification: unbounded predictions, poor calibration, and violations of core assumptions. These issues undermined interpretability and reliability, especially in threshold-sensitive contexts.

To address these challenges, I transitioned to logistic regression, which offered bounded probability outputs, clearer coefficient interpretation, and stronger theoretical alignment. Throughout the modeling process, I quantified misfit, crafted stakeholder-ready narratives to explain metric tradeoffs, and prioritized interpretability without sacrificing predictive strength.

## Github Repo and Source Data File

All project files are maintained in [this Github repository](https://github.com/dtminnick/income).

The UCI Adult Income dataset and related information are available for download from the [UCI archive site](https://archive.ics.uci.edu/dataset/2/adult).

## Code Libraries

My analysis leverages the following R packages: `caret` for model training and evaluation, `dplyr` and `tidyr` for data manipulation, `ggplot2` for plots, `knitr` for table formatting, and `pROC` for ROC/AUC analysis.

Two customized R functions enable evaluation of predictor associations and multicollinearity checks.

Code chunks are presented in this R markdown document alongside analysis to document implementation of analysis, model creation, and evaluation.

```{r, load_libraries}
library("caret")
library("dplyr")
library("ggplot2")
library("knitr")
library("pROC")
library("tidyr")

source("../R/check_multicollinearity_factors.R")
source("../R/test_predictor_associations.R")
```

# Summary of Data Exploration, Cleaning and Transformation (See Appendix A)

The modeling pipeline began with a comprehensive audit of the UCI Adult Income dataset to ensure readiness for classification. Key steps included:

**Exploratory Analysis:** Assessed distributions and relationships between predictors and income. Visualized key variables such as age, education, hours worked, and capital gains/losses.

**Missingness Check:** No missing data was present in the dataset, allowing for a streamlined preprocessing workflow without imputation.

**Feature Engineering:** Created derived variables like career_stage (age bins), consolidated education levels, and flagged investment activity using capital features.

**Data Cleaning:** Standardized categorical levels, pruned low-variance features, and ensured consistent formatting across factor variables to support design matrix alignment.

**Encoding Strategy:** Applied one-hot encoding to categorical variables, carefully managing reference levels to avoid multicollinearity and NA coefficients.

These steps established a clean, interpretable, and structurally sound dataset for model comparison and final evaluation.

# Correlations

Load transformed income data.

```{r load_transformed_data}
income <- readRDS("../data/income_final.rds")
```

Check for correlations among predictors.

```{r check_correlations}
# Set vectors.

predictors <- c("career_stage", "marital_status_group", "hours_group",
                "education_group", "has_investment_activity")

ordinal_vars <- c("career_stage", "hours_group", "education_group")

# Run the function to generate correlations summary.

assoc_summary <- test_predictor_associations(income, predictors, ordinal_vars)

rownames(assoc_summary) <- NULL
```

This correlation matrix confirms that the predictors are behaviorally distinct but lightly interrelated, which is ideal for modeling.

Print correlations in table form.

```{r show_correlation_values}
# Print numeric values.

kable(assoc_summary, 
      col.names = c("Pair", "Type", "Value"),
      format.args = list(big.mark = ","),
      align = c("l", "l", "r"))
```

`career_stage ~ marital_status_group`, with a Cramér’s V = 0.406, is the strongest association. This suggests that career progression is meaningfully related to marital status, possibly due to age, stability, or life stage effects. This could also reflect behavioral segmentation: married individuals may cluster in mid-career or peak-earning stages.

Cramér’s V values above 0.3 suggest moderate association between categorical variables. Spearman correlations are also low, indicating weak monotonic relationships between ordinal variables.

# Multicollinearity

Use the custom function `check_multicollinearity_factors` on the income dataset, using the specified predictors. Calculate Variance Inflation Factors (VIFs) to see how much each predictor is correlated with the others.

```{r check_multicollinearity}
predictors <- c("career_stage", "marital_status_group", "hours_group",
                "education_group", "has_investment_activity")

vif_summary <- check_multicollinearity_factors(income, predictors)

rownames(vif_summary) <- NULL

kable(vif_summary, 
      col.names = c("Variable", "VIF"),
      format.args = list(big.mark = ","),
      align = c("l", "r"))
```

All VIFs are below 2.2, which is comfortably within the safe zone (common thresholds are 5 or 10). This means no predictor is linearly dependent on the others, and coefficient estimates should be stable.

This VIF profile confirms that the model should be statistically sound and behaviorally distinct.

# Split Data

Split the dataset into 60% training, 20% validation, and 20% test. This allocation provides enough data to train stable models while dedicating a higher-than-usual share to validation and testing. With a large dataset, this approach strengthens model comparison, improves tuning, and ensures that final performance metrics are based on a robust and representative holdout set.

```{r split_data}
set.seed(123)

# Initial train/test split.

train_idx <- createDataPartition(income$income, p = 0.6, list = FALSE)

income_train <- income[train_idx, ]

temp  <- income[-train_idx, ]

# Split remaining into validation/test.

valid_idx <- createDataPartition(temp$income, p = 0.5, list = FALSE)

income_validate <- temp[valid_idx, ]

income_test <- temp[-valid_idx, ]
```

Check class balance in the train, validation and test sets.

```{r check_splits}
# Check balance function.

check_balance <- function(df, name) {
  df %>%
    count(income) %>%
    mutate(prop = round(n / sum(n), 2),
           dataset = name) %>%
    select(dataset,
           income, 
           n,
           prop)
}

# Generate data frame.

check <- bind_rows(check_balance(income_train, "Train"),
                   check_balance(income_validate, "Validation"),
                   check_balance(income_test, "Test"))

# Produce table summary.

kable(check,
      col.names = c("Dataset", "Income Level", "Count", "Percent"),
      caption = "Dataset Class Balance",
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r"))
```

The table confirms the income class split across train, validation, and test sets.

I will maintain the natural 75/25 class split in the dataset so the models reflect the real-world distribution of outcomes.

# Train Models

## Linear Regression Indicator Matrix Model

### Prepare Indicator Matrices

Prepare the predictor matrix X, containing numeric representations of age, education, and marital status. The response variable, income, is coded as 0 `<=$50K` or 1 `>$50K`. It is converted to a one-hot (indicator) matrix Y for the multivariate linear regression.

```{r}
# Create indicator matrix for binary response; assumed to be 0/1

G <- income_train$income_num  

# One-hot encoding

Y <- model.matrix(~ factor(G) - 1)
```

### Fit Linear Indicator Model

A multivariate linear regression is fit with the one-hot encoded response matrix. This approach models the probability of each class as a linear combination of predictors.

```{r}
# Fit the linear regression model.

model_linear_matrix <- lm(Y ~
                            poly(as.numeric(education_group), 2) +
                            splines::ns(as.numeric(career_stage), df = 3) +
                            marital_status_group +
                            has_investment_activity,
                          data = income_train)
```

This model predicts income classification using a blend of nonlinear and categorical predictors. It captures quadratic effects of education level, flexible spline-based trends across career stages, and categorical shifts tied to marital status and investment activity. 

By combining polynomial and spline transformations with group-level indicators, the model aims to uncover nuanced structural relationships while maintaining interpretability across key socioeconomic dimensions.

### Produce Linear Indicator Model Summary

Produce model summary.

```{r}
summary(model_linear_matrix)
```

For Class 0 (`<=50k`), the intercept is high (≈0.99), and education has a strong negative linear effect. Career stage splines show significant negative associations, while being married or having investment activity substantially lowers the predicted probability of income `<=50k`.

For Class 1 (`>50K`), the intercept is low (≈0.01), and the signs of all coefficients are reversed, as expected in a complementary indicator setup. Education, career stage, marital status, and investment activity all positively influence the likelihood of income `>50K`.

Both models show strong overall fit (adjusted R-squared of 0.343), highly significant predictors, and symmetric residual distributions, suggesting the indicator matrix is capturing meaningful structure.  The second-order education term and third spline basis are not significant, suggesting possible overparameterization.

### Plot Raw Residuals

Raw residual plots display the individual prediction errors for each observation, plotted against their fitted values. This helps assess whether the model's errors are randomly distributed or show patterns that suggest structural issues.

```{r}
# Capture fitted values and residuals.

income_train$mlm_resid_0 <- residuals(model_linear_matrix)[,1]
income_train$mlm_fitted_0 <- fitted(model_linear_matrix)[,1]
income_train$mlm_resid_1 <- residuals(model_linear_matrix)[,2]
income_train$mlm_fitted_1 <- fitted(model_linear_matrix)[,2]

# Convert to long form.

income_long <- income_train %>%
  select(mlm_fitted_0, mlm_resid_0, mlm_fitted_1, mlm_resid_1) %>%
  pivot_longer(
    cols = everything(),
    names_to = c(".value", "class"),
    names_pattern = "(mlm_fitted|mlm_resid)_(\\d)"
  ) %>%
  mutate(class = paste("Class", class),
         sqrt_abs_resid = sqrt(abs(mlm_resid)))

# Plot residuals.

ggplot(income_long, aes(x = mlm_fitted, y = mlm_resid)) +
  geom_point(alpha = 0.4, color = "steelblue") +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~ class) +
  labs(title = "Residuals vs Fitted by Class", x = "Fitted", y = "Residuals") +
  theme_minimal() + 
    theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
          plot.title = element_text(size = 14, face = "bold"))
```

In our case, the residuals form a funnel shape, indicating heteroscedasticity—where error variance increases with predicted probability. The smooth trend line also reveals nonlinearity, suggesting the model may be missing key interactions or transformations.

### Plot Binned Residuals

Binned residual plots aggregate residuals into intervals of fitted probabilities, averaging them to reduce noise and highlight systematic bias. This makes it easier to detect under- or over-prediction across the probability spectrum.

```{r}
# Produced binned residuals.

arm::binnedplot(x = fitted(model_linear_matrix),
         y = residuals(model_linear_matrix, type = "response"),
         xlab = "Fitted Probabilities",
         ylab = "Average Residuals",
         main = "Binned Residual Plot - Linear Indicator Model")
```

The binned residuals deviate from zero in a curved pattern, especially at the extremes, indicating that the model consistently mis-estimates probabilities in those regions. This reinforces the need for more flexible modeling to correct structural misfit and improve calibration.

Unbounded probabilities, i.e. the values predicted below 0 or above 1, are a clear sign of model misalignment when working with binary outcomes. In the context of a linear indicator model, this occurs because the model treats probabilities as continuous outputs without a constraining link function.

Despite its limitations, I will continue to use the linear indicator model for comparison purposes.

### Predict Class Scores

Predict class scores initially with training data.

```{r}
# Predict class scores.

train_pred <- predict(model_linear_matrix, newdata = income_train)
```

For each observation, assign the class with the highest predicted score. We then recode it back to match the original binary labels (0 or 1).

```{r}
# Assign predicted class (1 or 2).

train_class_pred <- max.col(train_pred)

# Recode predicted class to match binary response (0/1).

train_class_pred_binary <- ifelse(train_class_pred == 1, 0, 1)
```

### Generate Confusion Matrix

Evaluate the predicted classes against the true labels using a confusion matrix.

```{r}
# Evaluate classification performance.

cm_linear_matrix_train <- caret::confusionMatrix(factor(train_class_pred_binary), factor(income_train$income_num))

cm_linear_matrix_train
```

The linear indicator model achieves strong overall accuracy (82.5%) and high sensitivity (94.8%) for the majority class (Class 0), correctly identifying most true positives. However, its specificity is low (43.6%), indicating frequent misclassification of minority class cases. 

The Kappa score (0.44) reflects moderate agreement beyond chance, and the significant McNemar’s test suggests asymmetry in classification errors. While the model performs well in detecting the dominant class, its limited balance and specificity highlight challenges in handling class imbalance and underscore the need for complementary diagnostics.

### Extreme Probabilities

Although we can produce predicted classes and metrics, the predicted probabilities from a linear model are not constrained to 0-1. This can result in nonsensical probabilities, motivating the use of logistic regression for binary outcomes.

The table below summarizes the number and percentage of predicted probabilities from the linear regression model that fall outside the valid 0–1 range for each class. As expected, linear regression applied to a binary outcome can produce estimates below 0 or above 1, highlighting a limitation of this approach for classification tasks.

First, we extract the predicted probabilities for each class from the linear regression model. These probabilities represent the model’s estimated likelihood that each individual falls into the ≤$50K or >$50K income category.

```{r}
# Extract columns.

prob_under50k <- train_pred[, "factor(G)0"]

prob_over50k  <- train_pred[, "factor(G)1"]
```

To assess the appropriateness of linear regression for a binary outcome, we identify predictions that fall outside the valid probability range of 0 to 1. The table below shows the number and percentage of such predictions for each class.

```{r}
# Count out-of-bounds for each class.

out_under <- sum(prob_under50k < 0 | prob_under50k > 1)

out_over  <- sum(prob_over50k  < 0 | prob_over50k  > 1)

total <- nrow(train_pred)

# Summary table.

check_summary <- data.frame(Cclass = c("<=50K", ">50K"),
                            out_of_bounds = c(out_under, out_over),
                            total = total,
                            percent_out_of_bounds = round(100 * c(out_under, out_over) / total, 2))

kable(check_summary,
      col.names = c("Class", "Out of Bounds", "Total", "Percent Out of Bounds"),
      caption = "Out of Bounds Data for Each Class",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r"))
```

The plot below illustrates the distribution of predicted probabilities for both income classes. The dashed red lines mark the valid 0–1 probability range. Any predictions beyond these boundaries are not interpretable as probabilities, demonstrating why logistic regression is generally preferred for binary classification problems.

```{r}
# Convert to long format.

df_long <- as.data.frame(train_pred) %>%
  pivot_longer(cols = everything(), 
               names_to = "Class", 
               values_to = "Probability") %>%
  mutate(Class = if_else(Class == "factor(G)0", "<=50K", ">50K"))

# Plot.

ggplot(df_long, aes(x = Probability, fill = Class)) +
  geom_histogram(bins = 50, color = "white", position = "dodge") +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = 1, color = "red") +
  geom_vline(xintercept = 1, linetype = "dashed", linewidth = 1, color = "red") +
  labs(
    x = "Predicted Probability",
    y = "Count",
    title = "Distribution of Linear Regression Predicted Probabilities by Class"
  ) +
  scale_fill_manual(values = c("<=50K" = "steelblue", ">50K" = "lightsteelblue")) +
  theme_minimal() + 
    theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
          plot.title = element_text(size = 14, face = "bold"))
```

Together, these outputs provide a numeric indication and clear visual of the constraints of using linear regression for a categorical outcome, setting the stage for comparison with the logistic regression model.

Because linear regression is not bounded, the model produces predicted values below 0 and above 1, which are not valid probabilities. This creates problems for interpretation, since values like –0.2 or 1.3 cannot be meaningfully explained as likelihoods. It also complicates classification, as thresholding these outputs can distort decision rules, and the predictions themselves are not well calibrated to reflect true probabilities.

### Generate ROC Curve and AUC Metric

To evaluate the discriminatory power of the indicator regression model, extract the predicted probabilities for the positive class and used them as inputs to generate a Receiver Operating Characteristic (ROC) curve.

```{r}
# Column 2 corresponds to class 1 (income_num == 1).

score_class1 <- train_pred[, 2]

roc_obj_lm <- pROC::roc(response = income_train$income_num, predictor = score_class1)

# Plot ROC curve.

plot(roc_obj_lm, col = "steelblue", lwd = 2, main = "ROC Curve for Indicator Regression")
```

The blue line bows confidently toward the top-left corner, which indicates high sensitivity and specificity across thresholds. The shape suggests an AUC in the high 0.7s to low 0.8s.

The gray line represents random guessing. The model clearly outperforms this baseline, which confirms meaningful signal in the predictors.

```{r}
# AUC value.

pROC::auc(roc_obj_lm)
```

This AUC score indicates that the model can distinguish between `>50K` and `<=50K` earners with 87.7% accuracy across all thresholds.

## Logistic Regression

### Fit Logistic Regression Model

Build a logistic regression model to estimate income likelihood, using career stage, marital status, hours worked, education, and investment activity as predictors, with adjustments for weighting.

```{r}
# Produce logistic model.

model_logistic <- glm(income ~ 
                        poly(as.numeric(education_group), 2) +
                        splines::ns(as.numeric(career_stage), df = 3) +
                        marital_status_group +
                        has_investment_activity,
                      data = income_train, family = "binomial")
```

This logistic regression model predicts income classification using the same set of predictors as the linear indicator model. However, by using a binomial link, the model ensures fitted probabilities remain bounded between 0 and 1, offering coherent and interpretable outputs.

### Produce Logistic Regression Model Summary

Produce model summary.

```{r}
summary(model_logistic)
```

All predictors are highly significant, with education showing strong curvature and marital status, especially being married, exerting a substantial positive effect on income probability. 

The model achieves a notable reduction in deviance and a well-calibrated fit, with bounded, interpretable probabilities and a compact AIC of 13,971. 

Its structure mirrors the linear indicator model but benefits from coherent probabilistic outputs and improved diagnostic behavior.

### Plot Raw Residuals

In the raw Pearson residual plot, residuals are more symmetrically distributed around zero, with less pronounced funneling, suggesting reduced heteroscedasticity and better variance stability across fitted values.

```{r}
# Create a dataframe with fitted values and residuals.

resid_df <- data.frame(fitted = fitted(model_logistic),
                       residuals = residuals(model_logistic, type = "pearson"),
                       actual = income_train$income)

# Convert actual outcome to factor for faceting.

resid_df$class <- factor(resid_df$actual, levels = c(0, 1), labels = c("Class 0", "Class 1"))

# Plot residuals vs. fitted by class.

ggplot(resid_df, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.3, color = "blue") +
  geom_smooth(method = "loess", se = TRUE, color = "blue", linewidth = 0.8) +
  facet_wrap(~ class) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs Fitted by Class",
       x = "Fitted Values",
       y = "Pearson Residuals") +
  theme_minimal()
```

### Plot Binned Residuals

In the binned residual plot, residuals are more tightly clustered around the zero line (compared to the corresponding linear indicator plot), with minimal curvature and narrower confidence bands, indicating improved calibration and reduced bias.

```{r}
arm::binnedplot(x = fitted(model_logistic),
         y = residuals(model_logistic, type = "response"),
         xlab = "Fitted Probabilities",
         ylab = "Average Residuals",
         main = "Binned Residual Plot")
```

The logistic model’s bounded predictions and probabilistic framework yield cleaner diagnostics and more reliable interpretability across the probability spectrum.

### Predict Class Scores

Extract the predicted probabilities for the positive class.

```{r}
income_train$predicted_prob <- predict(model_logistic, type = "response")
```

### Generate Confusion Matrix

Evaluate the predicted classes against the true labels using a confusion matrix.

```{r}
threshold <- 0.5

income_train$predicted_class <- ifelse(income_train$predicted_prob > 0.5, 1, 0)

cm_logistic_train <- caret::confusionMatrix(factor(income_train$predicted_class),
                                            factor(income_train$income_num))

cm_logistic_train
```

The logistic model shows modest gains over the linear indicator model in classification performance. Accuracy improves slightly (83.3% vs. 82.5%), and balanced accuracy rises from 0.692 to 0.720, reflecting better handling of both classes. Sensitivity remains high for the majority class (Class 0), while specificity improves from 43.6% to 50.1%, indicating fewer false positives. The Kappa score also increases, suggesting stronger agreement beyond chance. Overall, the logistic model offers more coherent probability estimates and a better balance between detecting true positives and minimizing misclassification of the minority class.

### Generate ROC Curve and AUC Metric

Generate a Receiver Operating Characteristic (ROC) curve.

```{r}
roc_obj_log <- pROC::roc(income_train$income_num, income_train$predicted_prob)

plot(roc_obj_log, col = "steelblue", main = "ROC Curve for Logistic Regression")
```

This blue line bows toward the top-left corner, which indicates high sensitivity and specificity across thresholds. The shape suggests an AUC in the high 0.7s to low 0.8s, which is similar to the linear indicator model.

The model clearly outperforms this baseline, which confirms meaningful signal in the predictors.

```{r}
pROC::auc(roc_obj_log)
```

This AUC score indicates that the model can distinguish between `>50K` and `<=50K` earners with 88% accuracy across all thresholds, which is slightly better than the linear indicator model.

# Compare Models

## Training Data

### Confusion Matrix Comparison

A side-by-side confusion matrices reveal how linear and logistic models differ in classification behavior.

```{r}
# Extract confusion matrix tables.

cm_lm <- cm_linear_matrix_train$table

cm_lg <- cm_logistic_train$table

# Convert to data frames and add model labels.

df_linear <- as.data.frame(cm_lm)

df_linear$model <- "Linear"

df_logistic <- as.data.frame(cm_lg)

df_logistic$model <- "Logistic"

# Combine both into one tidy data frame.

df_combined <- rbind(df_linear, df_logistic)

# Rename columns for clarity.

colnames(df_combined) <- c("Predicted", "Actual", "Count", "Model")

# Add TP/FP/TN/FN label.

df_combined$Label <- with(df_combined, ifelse(
  
  Predicted == 1 & Actual == 1, "TP",
  
  ifelse(Predicted == 1 & Actual == 0, "FP",
         
  ifelse(Predicted == 0 & Actual == 0, "TN",
         
  "FN"))))

# Reorder columns for clarity.

df_final <- df_combined[, c("Model", "Predicted", "Actual", "Label", "Count")]

# View result.

kable(df_final,
      col.names = c("Model", "Predicted", "Actual", "Label", "Count"),
      caption = "Confusion Matrix Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r", "r"))
```

Compared to the linear indicator model, the logistic model shows a trade-off between false positives and true positives. While it slightly reduces true negatives (13,926 vs. 14,056), it captures more true positives (2,355 vs. 2,052) and fewer false negatives (2,350 vs. 2,653), indicating improved sensitivity. However, this comes with a modest increase in false positives (906 vs. 776). Overall, the logistic model offers better balance in classification, particularly in identifying the minority class more effectively.

### Performance Metric Comparison

To assess model performance beyond raw classification counts, we compare key evaluation metrics from linear and logistic regression, including accuracy, sensitivity, specificity, and predictive values.

```{r}
# Extract metrics from both slots.

overall_linear <- cm_linear_matrix_train$overall

byclass_linear <- cm_linear_matrix_train$byClass

overall_logistic <- cm_logistic_train$overall

byclass_logistic <- cm_logistic_train$byClass

# Select key metrics.

metrics_overall <- c("Accuracy", "Kappa")

metrics_byclass <- c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Balanced Accuracy")

# Build data frame.

df_metrics <- data.frame(metric = c(metrics_overall, 
                                    metrics_byclass),
                         linear_train = round(c(overall_linear[metrics_overall], 
                                          byclass_linear[metrics_byclass]), 4),
                         logistic_train = round(c(overall_logistic[metrics_overall], 
                                            byclass_logistic[metrics_byclass]), 4),
                         row.names = NULL)

# View result.

kable(df_metrics,
      col.names = c("Metric", "Linear", "Logistic"),
      caption = "Model Metric Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r"))
```

The logistic model outperforms the linear indicator model across most classification metrics. It shows higher overall accuracy (83.3% vs. 82.5%), stronger agreement beyond chance (Kappa: 0.49 vs. 0.44), and notably better specificity (50.1% vs. 43.6%), indicating improved detection of the minority class. While sensitivity remains high for both models, the logistic approach offers a more balanced and reliable classification profile, with gains in positive predictive value and balanced accuracy that reflect better calibration and class discrimination.

### AUC Comparison (Versus Accuracy and Kappa)

AUC measures how well a model ranks positives above negatives across all thresholds. It’s threshold-agnostic and robust to class imbalance. Accuracy is threshold-dependent and can be misleading if one class dominates. Kappa adjusts accuracy for chance agreement, but still depends on a fixed threshold.

AUC tells us how well the models discriminate, not just how often it’s “right” at a single cutoff. This is especially useful when comparing models with similar accuracy but different ranking behavior.

```{r}
# Extract AUCs.

auc_lm   <- pROC::auc(roc_obj_lm)

auc_log  <- pROC::auc(roc_obj_log)

# Create comparison data frame.

auc_comparison <- data.frame(Model = c("Linear", "Logistic"),
                             AUC = c(auc_lm, auc_log))

# View result.

kable(auc_comparison,
      col.names = c("Model", "AUC"),
      caption = "AUC Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r"))
```

The logistic model shows a slight edge in discriminative performance, with an AUC of 0.880 compared to 0.877 for the linear indicator model. While both models demonstrate strong ability to separate classes, the logistic approach offers marginally better ranking of predicted probabilities.

## Validation Data

### Linear Regression with an Indicator Matrix

#### Predict Class Scores

```{r}
# Predict class scores.

pred_valid <- predict(model_linear_matrix, newdata = income_validate, type = "response")
```

For each observation, assign the class with the highest predicted score. We then recode it back to match the original binary labels (0 or 1).

```{r}
# Assign predicted class (1 or 2).

class_pred_valid <- max.col(pred_valid)

# Recode predicted class to match binary response (0/1).

class_pred_binary_valid <- ifelse(class_pred_valid == 1, 0, 1)
```

#### Generate Confusion Matrix

Evaluate the predicted classes against the true labels using a confusion matrix.

```{r}
# Evaluate classification performance.

cm_linear_matrix_validate <- caret::confusionMatrix(factor(class_pred_binary_valid),
                                                    factor(income_validate$income_num))

cm_linear_matrix_validate
```

On the validation set, the linear indicator model maintains strong overall accuracy (82.4%) and high sensitivity (94.9%) for the majority class, effectively identifying true positives. However, specificity remains low (43%), indicating frequent misclassification of minority class cases. The Kappa score (0.44) reflects moderate agreement beyond chance, and the balanced accuracy of 0.689 highlights the model’s limited ability to handle class imbalance. Overall, the model performs reliably for dominant class detection but struggles with minority class discrimination.

### Logistic Regression

#### Predict Class Scores

Predict probabilities and class labels.

```{r}
# Predict probabilities for class 1 (income >50K).

pred_probs_logistic <- predict(model_logistic, newdata = income_validate, type = "response")

# Convert to binary class predictions using threshold 0.5.

class_pred_logistic <- ifelse(pred_probs_logistic >= 0.5, 1, 0)

```

#### Generate Confusion Matrix

Evaluate with confusion matrix.

```{r}

cm_logistic_validate <- caret::confusionMatrix(factor(class_pred_logistic),
                                            factor(income_validate$income_num))

cm_logistic_validate
```

On the validation set, the logistic model delivers improved classification performance over its linear counterpart. It achieves higher accuracy (83.5%) and a stronger Kappa score (0.49), indicating better agreement beyond chance. Sensitivity remains high (94.1%) for the majority class, while specificity improves to 50.1%, reflecting more balanced detection of minority cases. The model also shows gains in positive predictive value and balanced accuracy (72.1%), reinforcing its strength in both calibration and class discrimination.

### Compare Metrics From Validation Data

Add metrics to metrics data frame.

```{r}
# Extract metrics from both slots.

overall_linear <- cm_linear_matrix_validate$overall

byclass_linear <- cm_linear_matrix_validate$byClass

overall_logistic <- cm_logistic_validate$overall

byclass_logistic <- cm_logistic_validate$byClass

# Select key metrics.

metrics_overall <- c("Accuracy", "Kappa")

metrics_byclass <- c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Balanced Accuracy")

# Build data frame.

df_metrics_valid <- data.frame(
                         linear_valid = round(c(overall_linear[metrics_overall], 
                                          byclass_linear[metrics_byclass]), 4),
                         logistic_valid = round(c(overall_logistic[metrics_overall], 
                                            byclass_logistic[metrics_byclass]), 4),
                         row.names = NULL)

df_metrics <- cbind(df_metrics, df_metrics_valid) %>%
  select(metric,
         linear_train,
         logistic_train,
         linear_valid,
         logistic_valid)

# View result.

kable(df_metrics,
      col.names = c("Metric", "Linear Train", "Logistic Train", "Linear Validate", "Logistic Validate"),
      caption = "Model Metric Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r", "r"))
```

Across both training and validation sets, the logistic model consistently outperforms the linear indicator model in key classification metrics. It shows higher accuracy, stronger Kappa scores, and notably better specificity and balanced accuracy—indicating improved detection of the minority class without sacrificing performance on the majority class. While sensitivity remains high for both models, the logistic approach offers more reliable predictive balance and agreement, making it the more robust choice for calibrated classification.

# Choose Final Model

I chose the logistic model as the final model due to its consistently stronger performance across both training and validation sets. It offers higher accuracy, better class balance, and more reliable specificity, all while producing coherent, bounded probability estimates. These advantages make it a more interpretable and calibrated choice for classification, especially in the presence of class imbalance.

## Retrain Final Model

Retrain the final logistic regression model on the full training and validation data, then evaluate on the test set once. This gives the cleanest estimate of how the model will perform in deployment.

Combine train and validation sets.

```{r}
income_train <- income_train %>%
  select(-predicted_prob,
         -predicted_class,
         -mlm_resid_0,
         -mlm_resid_1,
         -mlm_fitted_0,
         -mlm_fitted_1)

income_full <- rbind(income_train, income_validate)
```

Produce the final model.

```{r}
# Produce logistic model.

# model_logistic_final <- glm(income_num ~ 
#                           poly(career_stage, 2) +
#                           marital_status_group +
#                           hours_group +
#                           splines::ns(as.numeric(education_group), df = 3) +
#                           has_investment_activity,
#                     data = income_full, weights = weight, family = "binomial")

# model_logistic_final <- glm(income ~ career_stage * hours_group +
#                               education_group * marital_status_group +
#                               has_investment_activity,
#                             data = income_full, weights = weight, family = "binomial")

model_logistic_final <- glm(income ~ 
                              poly(as.numeric(education_group), 2) +
                              splines::ns(as.numeric(career_stage), df = 3) +
                              marital_status_group +
                              has_investment_activity,
                            data = income_full, family = "binomial")

summary(model_logistic_final)
```

There is an increase in deviance and AIC compared to the prior logistic model.  This reflects the larger sample size, not a drop in model quality. The final model is trained on more data, which naturally increases total deviance but improves generalizability.

Coefficients remain stable across models, with minor shifts reflecting the added data. The career stage terms show stronger curvature, and the marital and hours effects slightly intensify, suggesting more robust estimates.

The final model preserves the structure and significance of the original while benefiting from a larger, more diverse training set. Coefficient magnitudes are consistent, and all predictors remain highly significant. The retrained model is better equipped to generalize, with more stable estimates and improved reliability for downstream evaluation.

## Evaluate Model

Run prediction using the final model with the test set.

```{r}
# Predict probabilities for class 1 (income >50K).

pred_probs_logistic_final <- predict(model_logistic_final, newdata = income_test, type = "response")

# Convert to binary class predictions using threshold 0.5.

class_pred_logistic_final <- ifelse(pred_probs_logistic_final >= 0.5, 1, 0)
```

Generate the confusion matrix for the final test set.

```{r}
cm_logistic_test <- caret::confusionMatrix(factor(class_pred_logistic_final),
                                            factor(income_test$income_num))

cm_logistic_test
```

On the unseen test set, the logistic model maintains strong and balanced performance. It achieves 83.6% accuracy with a Kappa of 0.50, indicating solid agreement beyond chance. Sensitivity remains high (93.9%) for the majority class, while specificity improves to 51.3%, reflecting better minority class detection. Balanced accuracy reaches 72.6%, confirming the model’s reliability and generalizability across class boundaries.

Add metrics to data frame.

```{r}
# Extract metrics from both slots.

overall_logistic <- cm_logistic_test$overall

byclass_logistic <- cm_logistic_test$byClass

# Select key metrics.

metrics_overall <- c("Accuracy", "Kappa")

metrics_byclass <- c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Balanced Accuracy")

# Build data frame.

df_metrics_test <- data.frame(
                         logistic_test = round(c(overall_logistic[metrics_overall], 
                                            byclass_logistic[metrics_byclass]), 4),
                         row.names = NULL)

df_metrics <- cbind(df_metrics, df_metrics_test) %>%
  select(metric,
         linear_train,
         logistic_train,
         linear_valid,
         logistic_valid,
         logistic_test)

# View result.

kable(df_metrics,
      col.names = c("Metric", 
                    "Linear Train", 
                    "Logistic Train", 
                    "Linear Validate", 
                    "Logistic Validate", 
                    "Logistic Test"),
      caption = "Model Metric Comparison",       
      format.args = list(big.mark = ","),
      align = c("l", "r", "r", "r", "r", "r"))
```

The logistic model consistently outperforms the linear indicator model across all datasets. It delivers higher accuracy, stronger Kappa scores, and notably better specificity and balanced accuracy—indicating improved minority class detection without compromising sensitivity. These gains hold steady from training through validation to the unseen test set, confirming the logistic model’s robustness, calibration, and generalizability for final deployment.

# Final Analysis

## Conclusions

This analysis compared linear and logistic regression for classifying income using socioeconomic predictors from the UCI Adult dataset. Logistic regression consistently outperformed the linear model across training, validation, and test sets, showing stronger accuracy, balance, and agreement beyond chance.

Its bounded probabilities and alignment with binary classification theory made it more interpretable and reliable, especially for detecting minority-class cases. While the linear model was competitive, it suffered from unbounded outputs and thresholding limitations.

Logistic regression was selected as the final model for its predictive strength, theoretical coherence, and suitability for transparent reporting and fairness auditing.

## Limitations of Linear Regression as a Classifier

Linear regression can be adapted for binary classification via thresholding, but it presents several key limitations:

**Unbounded predictions:** Outputs are not restricted to the [0, 1] range, making probability interpretation invalid and thresholding arbitrary.

**Assumption violations:** Linear regression relies on homoscedasticity and normally distributed errors, conditions that rarely hold in classification settings, leading to biased estimates and unreliable inference.

**Poor calibration:** Predicted values do not correspond to true class probabilities, weakening decision-making in threshold-sensitive contexts.

**Lack of log-odds interpretation:** Coefficients lack the intuitive odds-based framing that logistic regression provides, complicating stakeholder communication.

These limitations motivated my transition to logistic regression, which offers bounded, interpretable outputs and better theoretical alignment with classification tasks.

## Challenges and Solutions

| Challenge | Solution | 
| --------- | -------- |
| Unbounded predictions from linear regression | Quantified the extent of unbounded outputs and highlighted the model’s misalignment with binary classification, reinforcing the need for bounded alternatives. |
| Communicating metric tradeoffs to stakeholders | Developed concise, audience-aware narratives emphasizing practical impact (e.g., “Logistic regression better identifies low-income individuals without sacrificing accuracy”). |
| Balancing interpretability with predictive strength | Chose logistic regression for its bounded outputs, theoretical coherence, and transparent coefficient-based insights into income drivers. |

# Appendix A: Data Exploration, Cleaning and Transformation

Exploratory analysis focuses on understanding five variables in the source data that will be transformed for analysis purposes. Three of the variables have a categorical version and numerical version to aid exploratory analysis.

Load the income data and select in-scope variables. Create a numeric version of the response variable.

```{r load_data}
income <- readRDS("../data/income.rds") %>%
  select(income,
         age,
         marital_status,
         hours_per_week,
         education, 
         capital_gain, 
         capital_loss) %>%
  mutate(income_num = if_else(income == "<=50K", 0, 1))
```

## Missing Values

Generate a report summarizing missing values in the dataset at both the column and row level. Compute the total number and percentage of rows containing any missing values. And combine summaries into a single table for easy inspection and reporting.

```{r missing_data}
# Create column level summary.

col_missing <- income %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "missing_count") %>%
  mutate(missing_percent = round(missing_count / nrow(income), 2))

# Create row level summary.

row_missing <- tibble(variable = "rows_with_missing_data",
                      missing_count = sum(!complete.cases(income)),
                      missing_percent = round(sum(!complete.cases(income)) / nrow(income), 2))

# Combine summaries.

missing_report <- bind_rows(col_missing, row_missing)

# Generate formatted report.

kable(missing_report,
      col.names = c("Variable", "Missing Count", "Missing Percent"),
      format.args = list(big.mark = ","),
      align = c("l", "r", "r"))
```

The table confirms there are no rows with missing data in the reported rows and columns.  The source dataset contains 32,560 observations.

## Income (Response)

`income` is a binary categorical variable indicating whether an individual's annual income exceeds $50,000. It has two levels: `<=50K` for those earning $50,000 or less, and `>50K` for those earning more than $50,000.  

`income_num` is a numeric variable that corresponds to `income`, i.e. 0 represents the `<=50k` category and 1 represents the `>50k` category.

```{r eda_income}
# Generate a summary of the income variable.

income_summary <- income %>%
  group_by(income) %>%
  summarise(entries = n()) %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot the distribution of values.

ggplot(income_summary, aes(x = income, y = entries)) +
  geom_col(fill = "steelblue", color = "white") +
  geom_text(aes(label = paste0(scales::comma(entries), "\n", round(percent * 100, 0), "%")),
            vjust = -0.3, size = 3.0) +
  labs(title = "Distribution of Income",
       x = "Income",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(income_summary$entries) * 1.2)
```

This distribution is a classic case of class imbalance and it’s substantial: 76% of individuals fall into the `<=50K` category, while only 24% belong to the `>50K` category.

Given the potential baseline accuracy trap, i.e. because the `<=50K` class dominates the dataset at 76%, I can achieve high accuracy simply by predicting the majority class every time. But this doesn't provide a meaningful model.

I need to also measure how well the models detect the minority class, ensure that I am not over-predicting the majority class, and generate models with overall discriminatory power.  I will use weights as a means to balance the classes before training the models.

Encode factor levels for income, using `<=50K` as the reference level.

```{r}
# Make response variable a factor.

income <- income %>%
  mutate(income = factor(income, levels = c("<=50K", ">50K")),
         income_num = factor(income_num, levels = c(0, 1)))
```

## Age

`age` is a continuous numeric variable, ranging from 17 to 90.  The following plot generates a summary of the `age` variable.

```{r}
# Generate a summary of the age variable.

age_summary <- income %>%
  group_by(age) %>%
  summarise(entries = n()) %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot the distribution of values.

ggplot(age_summary, aes(x = age, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Age",
       x = "Age",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

This distribution reflects what we'd expect from a working-age population in the US: a peak around ages 20-30, reflecting a large cohort entering the workforce, a gradual decline beginning at age 37-38 through age 55, typical of aging out of peak earning years or shifting to preparation for retirement, and dropoff after ages 60-50, reflecting retirement and reduced representation.

Age is likely nonlinear in its relationship to income class.  Use binning to confirm this dynamic.

```{r}
# Create age bins.

income_age <- income %>%
  mutate(age_bin = cut(age, breaks = seq(15, 90, by = 5), include.lowest = TRUE))

# Calculate proportions within each age bin and income group.

age_income_prop <- income_age %>%
  group_by(age_bin, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(age_bin) %>%
  mutate(prop = count / sum(count))

# Generate plot.

ggplot(age_income_prop, aes(x = age_bin, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Proportion of Income Classes by Age Group",
       x = "Age Group",
       y = "Proportion",
       fill = "Income Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

This plot shows that income probability doesn't increase linearly with age; rather, it peaks mid-career and then drops.  I'll use a binning strategy to model this non-monotonic pattern in a `career_stage` variable, i.e. instead of assuming that each year of age adds the same effect, binning allows me to treat age as a set of behavioral groups. To better capture this pattern, I will engineer a career stage feature.

## Career Stage

The `career_stage` feature will contain four groups: 1) entry-level (ages 17-30), 2) growth phase (ages 31-44), 3) peak earning (ages 45-60), and 4) retirement transition (ages 61+).  Encode factor levels for career stage, using Entry-level as the reference level.

```{r define_career_stage}
# Create career_stage variable.

income <- income %>%
  mutate(career_stage = case_when(
    age < 31 ~ "Entry-level",
    age >= 31 & age < 45 ~ "Growth-phase",
    age >= 45 & age < 61 ~ "Peak-earning",
    age >= 61~ "Retirement-transition"))

# Make variable a factor.

income <- income %>%
  mutate(career_stage = factor(career_stage, 
                               ordered = TRUE,
                               levels = c("Entry-level", 
                                          "Growth-phase", 
                                          "Peak-earning", 
                                          "Retirement-transition")))
```

Visually confirm the non-linear effects of career stage on income classification.

```{r}
# Group by career stage.

career_stage_prop <- income %>%
  group_by(career_stage, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(career_stage) %>%
  mutate(prop = count / sum(count))

# Generate plot.

ggplot(career_stage_prop, aes(x = career_stage, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Proportion of Income Classes by Career Stage",
       x = "Career Stage",
       y = "Proportion",
       fill = "Income Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

To capture this nonlinear effect, I will test a second-degree orthogonal polynomial transformation of the numeric career stage variable as `poly(career_stage_num, 2)`.  It will model both linear and quadratic trends while avoiding multicollinearity.

There is a single inflection point in this distribution. The chart shows a rise in `>50K` income class from Entry-level to Peak-earning, followed by a decline in Retirement-transition. That’s a classic parabolic shape with one turning point which a quadratic models naturally.

This should improve numerical stability and interpretability of regression coefficients, especially when modeling non-monotonic relationships (e.g., mid-career income peaks followed by retirement declines).

## Marital Status

The marital status variable captures an individual's relationship status and is a categorical feature with the following levels:

* Married-civ-spouse: Legally married and living with spouse,
* Married-AF-spouse: Married to a spouse in the armed forces,
* Divorced: Legally separated after marriage,
* Separated: Still legally married but not living together,
* Widowed: Spouse has passed away, and
* Never-married.

Generate a summary of `marital_status`.

```{r}
# Generate summary.

marital_status_summary <- income %>%
  group_by(marital_status) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot distribution of values.

ggplot(marital_status_summary, aes(x = forcats::fct_reorder(marital_status, entries, .desc = TRUE), y = entries)) +
  geom_col(fill = "steelblue", color = "white") +
  geom_text(aes(label = paste0(scales::comma(entries), "\n", round(percent * 100, 0), "%")),
            vjust = -0.3, size = 3.0) +
  labs(title = "Distribution of Marital Status",
       x = "Marital Status",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(marital_status_summary$entries) * 1.2)
```

Married-civ-spouse dominates at 46%.  Never-married follows at 33%.  The rest of the categories are smaller slices, but potentially behaviorally distinct.

Marital status could be a strong socioeconomic signal when predicting income, e.g. married individuals may benefit from dual incomes or household stability.  Never-married or separated individuals might reflect different life stages or economic pressures.

These categories are not ordinal, e.g. widowed or married is not more or less than divorced.

Collapsing sparse categories may improve model stability and interpretability, e.g. married, non-traditional married, previously married, never married.

## Marital Status Group

Marital status group collapses sparse categories into broader groups for model stability and interpretability.

```{r define_marital_group}
# Create marital status group variable

income <- income %>%
  mutate(marital_status_group = case_when(
    marital_status %in% c("Married-civ-spouse", 
                          "Married-AF-spouse") ~ "Married",
    marital_status %in% c("Divorced", 
                          "Separated", 
                          "Married-spouse-absent", 
                          "Widowed") ~ "Previously-married",
    marital_status == "Never-married" ~ "Never-married"))

# Make variable a factor.

income <- income %>%
  mutate(marital_status_group = factor(marital_status_group, 
                                       ordered = FALSE,
                                       levels = c("Never-married", 
                                                  "Previously-married", 
                                                  "Married")))
```

Show distribution of income by marital status group.

```{r}
# Group by marital group.

marital_status_group_prop <- income %>%
  group_by(marital_status_group, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(marital_status_group) %>%
  mutate(prop = count / sum(count))

# Generate plot.

ggplot(marital_status_group_prop, aes(x = marital_status_group, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Proportion of Income Classes by Marital Status Group",
       x = "Marital Status Group",
       y = "Proportion",
       fill = "Income Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

Married individuals show the highest proportion of income earners in the `>50K` category. This suggests that marriage may correlate with financial stability, dual-income households, or career maturity.

Previously-married and Never-married groups lean heavily toward the `<=50K` category, indicating lower income prevalence.

## Hours Per Week

Hours per week represents the number of hours an individual reports working per week in their primary job.

```{r}
hours_per_week_summary <- income %>%
  group_by(hours_per_week) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

ggplot(hours_per_week_summary, aes(x = hours_per_week, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Hours Per Week",
       x = "Hours Per Week",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(hours_per_week_summary$entries) * 1.2)
```

There is a prominent spike at 40 hours, which can be considered a classic full-time benchmark in the US.  It dominates the dataset, reflecting standard employment contracts.  There are smaller peaks at 20, 30, 50 and 60 hours, suggesting common part-time and overtime thresholds.  These are potentially tied to specific industries or roles.  The jagged, irregular tails hint at self-reported data or job-specific norms, e.g. self-employment or gig work.

## Hours Group

The hours group variable is a derived categorical feature that segments individuals based on their reported weekly work hours:

* Underemployed: Works fewer than 30 hours per week,
* Full-time: Works between 30 and 45 hours per week, and
* Overtime: Works more than 45 hours per week.

```{r define_hours_group}
# Create hours group variable.

income <- income %>%
  mutate(hours_group = case_when(
    hours_per_week < 30 ~ "Underemployed",
    hours_per_week >= 30 & hours_per_week <= 45 ~ "Full-time",
    hours_per_week > 45 ~ "Overtime"))

# Make variable a factor.

income <- income %>%
  mutate(hours_group = factor(hours_group,
                              ordered = TRUE,
                              levels = c("Underemployed", 
                                         "Full-time", 
                                         "Overtime")))
```

Show proportion of high earners by hours group.

```{r}
hours_group_prop <- income %>%
  group_by(hours_group, income) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(hours_group) %>%
  mutate(prop = count / sum(count))
  
ggplot(hours_group_prop, aes(x = hours_group, y = prop, fill = income)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  labs(
    title = "Income Proportion by Hours Group",
    x = "Hours Group",
    y = "Proportion",
    fill = "Income Level"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
        plot.title = element_text(size = 14, face = "bold"))
```

The underemployed group is overwhelmingly low income and likely includes part-time, seasonal or precarious workers.  The full-time group is majority `<=$50k` but has a noticeable uptick in high earners.  This suggests that full-time work alone isn't a guarantee of higher income.  The overtime group has the most balanced distribution; it probably includes a mix of skilled labor, self-employed individuals and salaried workers with performance incentives.

## Education

Education is a categorical variable with 16 levels, ranging from Preschool through advanced degrees like Doctorate` and Prof-school.

```{r}
# Make variable a factor.

income <- income %>%
  mutate(education = factor(education, 
                            levels = c("Preschool", 
                                       "1st-4th", 
                                       "5th-6th", 
                                       "7th-8th", 
                                       "9th", 
                                       "10th", 
                                       "11th", 
                                       "12th",
                                       "HS-grad", 
                                       "Some-college", 
                                       "Assoc-voc", 
                                       "Assoc-acdm",
                                       "Bachelors",
                                       "Masters",
                                       "Prof-school", 
                                       "Doctorate")))

education_summary <- income %>%
  group_by(education) %>%
  summarise(entries = n()) %>%
  mutate(percent = round(entries / sum(entries), 2))

ggplot(education_summary, aes(x = education, y = entries)) +
  geom_col(fill = "steelblue", color = "white") +
  geom_text(aes(label = paste0(scales::comma(entries), "\n", round(percent * 100, 0), "%")),
            vjust = -0.3, size = 3.0) +
  labs(title = "Distribution of Education",
       x = "Education",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(education_summary$entries) * 1.2)
```

High school graduates dominate the sample at 32%; this aligns with national trends.  Some-college and Bachelors categories together make up 40% of the sample, indicating strong representation.  Advanced degrees are relatively rare; combined they account for ~7% of the sample, which could limit model ability to generalize to highly educated groups.  Low education levels are sparse; these are likely to be older adults or immigrants with limited formal schooling.

Plot income proportions by education.

```{r}
# Group by education.

income_education_prop <- income %>%
  group_by(education, income) %>%
  summarise(entries = n(), .groups = "drop") %>%
  group_by(education) %>%
  mutate(prop = entries / sum(entries))

# Plot summary.

ggplot(income_education_prop, 
       aes(x = education, 
           y = prop, 
           fill = income)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Proportion by Education Level",
       x = "Education Level",
       y = "Proportion",
       fill = "Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

There's an unmistakable pattern evident in this chart: the gradient from low education levels to advanced degrees is a textbook example of socioeconomic stratification.  Still its surprising to see how nonlinear the education payoff is in income potential.

The proportion of individuals earning `>50K` doesn’t increase at a constant rate across education levels. Instead, it jumps more sharply at certain thresholds, especially from HS-grad to Bachelors, and again from Masters to Doctorate.

High income really starts to dominate at the Bachelor level and is an inflection point for income potential.  Advanced degrees show the highest income potential, but this comes with investment of time and potentially debt as well.

I'll use ordinal coding to treat education as a ranked factor to capture the income gradient.  I'll create a separate `education_group` variable for this as part of transformations.

## Education Group

Create a separate `education_group` variable.

```{r define_education_level_cat}
# Create education level variable.

income <- income %>%
  mutate(education_group = case_when(
            education %in% c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", 
                                 "10th", "11th", "12th") ~ "Non-HS",
            education %in% c("HS-grad", "Some-college", 
                                 "Assoc-voc", "Assoc-acdm") ~ "HS-some-college",
            education %in% c("Bachelors") ~ "Bachelors",
            education %in% c("Masters") ~ "Masters",
            education %in% c("Prof-school", "Doctorate") ~ "Prof-doctorate"))

# Make variable a factor.

income <- income %>%
  mutate(education_group = factor(education_group,
                                  ordered = TRUE,
                                  levels = c("Non-HS", 
                                             "HS-some-college", 
                                             "Bachelors",
                                             "Masters",
                                             "Prof-doctorate")))
```

Plot proportion of income by education group.

```{r}
# Group by education group.

education_group_prop <- income %>%
  group_by(education_group, income) %>%
  summarise(entries = n(), .groups = "drop") %>%
  group_by(education_group) %>%
  mutate(prop = entries / sum(entries))

# Plot summary.

ggplot(education_group_prop, 
       aes(x = education_group, 
           y = prop, 
           fill = income)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Proportion by Education Level",
       x = "Education Level",
       y = "Proportion",
       fill = "Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

The income jump from Bachelors to Masters to Prof-doctorate is not linear. The gains accelerate, suggesting credential thresholds, e.g., graduate degrees unlocking higher-paying roles, and labor market segmentation, e.g., professional degrees tied to elite occupations.

I will test a natural cubic splines to model this non-linear relationship between your ordinal education variable and the outcome, i.e. `ns(as.numeric(education_group), df = 3)`.

## Capital Gain/Loss

In the UCI Adult Income dataset, the capital gain and capital loss variables don’t represent individual investment transactions like you’d see in a brokerage account. Instead, they are annual amounts reported on tax returns.

Here’s what they represent in practice:

**Capital Gain:** The total taxable profit someone reported in a year from selling assets (stocks, bonds, property, etc.) for more than they paid.

**Capital Loss:** The total deductible loss someone reported in a year from selling assets for less than they paid. Tax rules allow limited reporting of such losses.

In the dataset, most people have zeros for both variables, meaning they didn’t report any gains or losses that year. Nonzero values are relatively rare but signal engagement with investment activity beyond wages.

### Limitation of Capital Gain/Loss as a Predictors

While capital gain and capital loss provide useful signals of investment activity, they capture only realized transactions (profits or losses from assets actually sold). They do not account for asset ownership or wealth holdings that have not been sold.  For example, someone may hold significant investments in real estate or retirement accounts but report zero gains or losses if they did not sell anything that year. 

This means the variables reflect investment activity and non-wage income, not necessarily investment capacity or wealth accumulation.

### Capital Gain

Generate capital gain summary.

```{r}
# Generate summary.

capital_gain_summary <- income %>%
  group_by(capital_gain) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot summary.

ggplot(capital_gain_summary, aes(x = capital_gain, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Capital Gain",
       x = "Capital Gain (Dollars)",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(capital_gain_summary$entries) * 1.2)
```

Capital gains are heavily zero-inflated, and that skew makes it a prime candidate for binary transformation. Most individuals have zero capital gain, so the continuous values only apply to a small subset. Continuous skewed variables can distort coefficients or inflate variance in linear models. A binary flag captures the presence of investment activity, which may correlate with income or occupation.

Plot cumulative distribution of capital gains.

```{r}
# Group by gain.

capital_gain_prop <- capital_gain_summary %>%
  arrange(capital_gain) %>%
  mutate(cumulative = cumsum(entries) / sum(entries))

# Plot proportion.

ggplot(capital_gain_prop, aes(x = capital_gain, y = cumulative)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Cumulative Distribution of Capital Gain",
       x = "Capital Gain (Dollars)",
       y = "Proportion") +
    theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

The cumulative distribution shows that over 92% of individuals report zero or minimal capital gains, with only a small fraction earning substantial amounts (e.g., >10,000).

Capital gain is a strong signal of financial activity and wealth accumulation, but it's concentrated among a small subset of the population.

### Capital Loss

Generate capital loss summary.

```{r}
# Generate summary.

capital_loss_summary <- income %>%
  group_by(capital_loss) %>%
  summarise(entries = n(), .groups = "drop") %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot summary.

ggplot(capital_loss_summary, aes(x = capital_loss, y = entries)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Distribution of Capital Loss",
       x = "Capital Loss (Dollars)",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  expand_limits(y = max(capital_loss_summary$entries) * 1.2)
```

Capital loss is also heavily zero-inflated.

Plot cumulative distribution of capital losses.

```{r}
# Group by loss.

capital_loss_prop <- capital_loss_summary %>%
  arrange(capital_loss_summary) %>%
  mutate(cumulative = cumsum(entries) / sum(entries))

# Plot proportion.

  ggplot(capital_loss_prop, aes(x = capital_loss, y = cumulative)) +
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "Cumulative Distribution of Capital Loss",
       x = "Capital Loss (Dollars)",
       y = "Proportion") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"))
```

Over 96% of individuals report zero or minimal capital loss. A steep rise in the cumulative distribution between 1,000 and 2,000 suggests that most reported losses fall within this range. Very few individuals report losses above 2,000.

Combining capital gain and capital loss into a single binary variable reduces two sparse, skewed variables into one binary flag. It preserves the signal that captures whether the individual has any investment-related income or loss. It is also easier to explain and visualize in behavioral segmentation or fairness audits.

## Has Investment Activity

Create a single investment activity flag using capital gain and capital loss.

```{r define_has_investment_activity}
# Create investment activity variable.

income <- income %>%
  mutate(has_investment_activity = if_else(capital_gain > 0 | capital_loss > 0, "Yes", "No"))

# Make variable a factor.

income <- income %>%
  mutate(has_investment_activity = factor(has_investment_activity,
                                          ordered = FALSE,
                                          levels = c("No", "Yes")))
```

Check distribution of the variable.

```{r}
# Group by activity.

investment_by_income <- income %>%
  group_by(has_investment_activity, income) %>%
  summarise(entries = n()) %>%
  mutate(percent = round(entries / sum(entries), 2))

# Plot distribution.

ggplot(investment_by_income, 
       aes(x = has_investment_activity, 
           y = percent, 
           fill = income)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("<=50K" = "lightsteelblue", ">50K" = "steelblue")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Proportion by Investment Activity",
       x = "Has Investment  Activity",
       y = "Proportion",
       fill = "Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
        plot.title = element_text(size = 14, face = "bold"))
```

Individuals who have not had capital gains or losses are predominantly in the `<=50K` income category.

Those who have reported capital gains or losses show a much higher proportion in the `>50K` category, suggesting that investment behavior correlates with higher earnings.

Investment activity may reflect financial literacy, risk tolerance, or access to disposable income.

It could also signal career stage or education level, since those with more resources and knowledge are more likely to invest.

The plot shows a strong separation in income proportions based on investment behavior. Binary encoding preserves that contrast cleanly.

Save data with features.

```{r save_transformed_data}
saveRDS(income, "../data/income_final.rds")
```